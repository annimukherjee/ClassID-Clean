{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1bf94-25d4-4ed1-bddd-bc3309763692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "# import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\", context=\"paper\")\n",
    "from cycler import cycler\n",
    "import os, sys\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import combinations\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO as _BytesIO\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.metrics import *\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "import traceback\n",
    "from sympy import Point, Polygon\n",
    "import mmcv\n",
    "# import mmcv\n",
    "\n",
    "\n",
    "import cv2\n",
    "from mmtrack.apis import inference_mot, init_model as init_tracking_model\n",
    "from smartprint import smartprint as sprint\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "def printm(s): return display(Markdown(s))\n",
    "    \n",
    "SERVER_CACHE_DIR = '/mnt/ci-nas-cache/edulyzeV2/cache_compute_4/fixed_face'\n",
    "os.makedirs(SERVER_CACHE_DIR,exist_ok=True)\n",
    "\n",
    "track_analysis_meta_cache = f'{SERVER_CACHE_DIR}/analysis_tracking/meta_info'\n",
    "os.makedirs(track_analysis_meta_cache,exist_ok=True)\n",
    "\n",
    "base_dir = '/mnt/ci-nas-cache/edulyzeV2/pose_face_gaze_emb_fixed_face'\n",
    "\n",
    "track_analysis_session_data = f'{SERVER_CACHE_DIR}/analysis_tracking/session_tracking_info'\n",
    "os.makedirs(track_analysis_session_data,exist_ok=True)\n",
    "\n",
    "postprocessed_id_map_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/processed_id_maps'\n",
    "os.makedirs(postprocessed_id_map_data_dir, exist_ok=True)\n",
    "\n",
    "id_viz_cache_root = f'{SERVER_CACHE_DIR}/analysis_tracking/session_matching_info'\n",
    "os.makedirs(id_viz_cache_root, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7da07-3043-4ac0-ace1-8adbf335aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_filter_list = [\n",
    "#  'classinsight-cmu_05681A_ghc_4301_201905011630',\n",
    "#  'classinsight-cmu_05681A_ghc_4301_201904171630',\n",
    "#  'classinsight-cmu_05681A_ghc_4301_201902201630',\n",
    "#  'classinsight-cmu_05681A_ghc_4301_201904101630',\n",
    "#  'classinsight-cmu_05681A_ghc_4301_201901231630',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201902251200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904081200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201905011200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904291200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904011200',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201902141630',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201904021630',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201902051630',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201902281630',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201903071630',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904230930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201903260930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904160930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904300930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201903190930',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904151500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201902251500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904081500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904221500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201902181500',\n",
    "                       \n",
    " # 'classinsight-cmu_17214B_ph_a21_201902271030',\n",
    " # 'classinsight-cmu_17214B_ph_a21_201903061030',\n",
    " # 'classinsight-cmu_17214B_ph_a21_201904031030',\n",
    " # 'classinsight-cmu_17214B_ph_a21_201904101030',\n",
    " # 'classinsight-cmu_17214B_ph_a21_201904241030',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201903201130',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201904101130',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201904171130',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201904241130',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201905011130',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201902111500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201903181500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904081500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904151500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904221500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201901281500'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae5c4b-ae69-4d6b-9d54-557dfee4c306",
   "metadata": {},
   "source": [
    "# Cache and get tracking information for sessions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed70816-015c-4598-8046-1ca5377bfa46",
   "metadata": {},
   "source": [
    "## Get frame file data for all sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068d7d9-2363-4909-80c8-4ee878d3d695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "frame_file_data = {}\n",
    "for course_idx, course_dir in enumerate(glob.glob(f\"{base_dir}/*\")):\n",
    "    course_name = course_dir.split(\"/\")[-1]\n",
    "    course_cache_file = f\"{track_analysis_meta_cache}/{course_name}\"\n",
    "    if os.path.exists(course_cache_file):\n",
    "        frame_file_data[course_name] = pickle.load(open(course_cache_file,\"rb\"))\n",
    "        continue\n",
    "    frame_file_data[course_name]={}\n",
    "        \n",
    "    for session_idx, session_dir in enumerate(glob.glob(f\"{course_dir}/*\")):\n",
    "        session_name = session_dir.split(\"/\")[-1]\n",
    "        frame_file_data[course_name][session_name] = {}\n",
    "        frame_files = glob.glob(f\"{session_dir}/*\")\n",
    "        frame_file_names = [xr.split(\"/\")[-1] for xr in frame_files]\n",
    "        if 'end.pb' in frame_file_names:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=True\n",
    "        else:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=False            \n",
    "        frame_ids = [int(xr.split(\".\")[0]) for xr in frame_file_names if not (xr=='end.pb')]\n",
    "        frame_file_data[course_name][session_name]['frame_ids'] = sorted(frame_ids)\n",
    "        frame_file_data[course_name][session_name]['dir_location'] = session_dir\n",
    "        print(f\"Got metadata for course: {course_idx}-{course_name}, session:{session_idx}-{session_name}\")\n",
    "    pickle.dump(frame_file_data[course_name],open(course_cache_file,\"wb\")) \n",
    "        \n",
    "        \n",
    "frame_file_data.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517b9df-a20b-4eac-b2ed-3efef81ee43c",
   "metadata": {},
   "source": [
    "## Collect tracking data for all sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae931e-d099-4637-92e7-9ce11623bda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writing a generic loop to get only-tracking info from all courses in frame file data, but putting a filter ahead\n",
    "\n",
    "for course_idx, course in enumerate(frame_file_data):\n",
    "    for session_idx, session_id in enumerate(frame_file_data[course]):\n",
    "        session_tracking_cache_file = f\"{track_analysis_session_data}/{session_id}.pb\"\n",
    "        # if session_id.split(\"-front\")[0] not in session_filter_list:\n",
    "        #     print(f\"Session {session_id} not in session filter list, skipping...\")\n",
    "        #     continue\n",
    "        try:\n",
    "a                print(f\"Got tracking info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            else:\n",
    "                ...\n",
    "                print(f\"FILE EXISTS: tracking info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "        except:\n",
    "            print(f\"ERROR: Unable to get session tracking for: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            unfinished_sessions.append((course, session_id))\n",
    "            print(traceback.format_exc())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bbcd1-d9c5-401f-a2dc-d9ed115f0daf",
   "metadata": {},
   "source": [
    "## Postprocess mmtrack results and create a new tracking map from older tracking ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc0025-35e5-4c7e-8f30-bc174f31bafe",
   "metadata": {},
   "source": [
    "### Run id filtering and mapping for all sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5d8da-4bd2-41da-ad90-a5305ce8980e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for course_idx, course in enumerate(frame_file_data):\n",
    "    for session_idx, session_id in enumerate(frame_file_data[course]):\n",
    "        session_preprocessed_id_map_file = f\"{postprocessed_id_map_data_dir}/{session_id}.pb\"\n",
    "        if not os.path.exists(session_preprocessed_id_map_file):\n",
    "            session_tracking_cache_file = f\"{track_analysis_session_data}/{session_id}.pb\"\n",
    "            session_frame_dir = f'/mnt/ci-nas-cache/edulyzeV2/pose_face_gaze_emb_fixed_face/{course}/{session_id}'\n",
    "            if not os.path.exists(session_tracking_cache_file):\n",
    "                printm(f\"## Tracking file does not exists for session {session_id}, skipping id-matching...\")\n",
    "                continue\n",
    "            \n",
    "            df_tracking = pickle.load(open(session_tracking_cache_file,\"rb\")).transpose()\n",
    "            printm(f'# {course_idx}-{course}, session:{session_idx}-{session_id}')\n",
    "            printm(f'## Raw tracking shape:{df_tracking.shape}')\n",
    "            \n",
    "            \n",
    "            # Find other IDs that start/end within 900 frames\n",
    "            printm(f'## Filter non-persistentids')\n",
    "            MIN_ID_FRAMES = 900 # number of frames an id needs to be a persistent id\n",
    "            col_start_stop_idxs = []\n",
    "            for col in df_tracking.columns:\n",
    "                one_idxs = df_tracking.index[np.where(df_tracking[col]==1)[0]].values\n",
    "                col_start_stop_idxs.append([col, one_idxs.min(), one_idxs.max()])\n",
    "            df_id_start_stop = pd.DataFrame(col_start_stop_idxs, columns=['id','min_idx','max_idx'])\n",
    "            df_id_start_stop['total_idxs'] = df_id_start_stop['max_idx']-df_id_start_stop['min_idx']\n",
    "            nonpersistent_ids_removed = df_id_start_stop[df_id_start_stop.total_idxs<=MIN_ID_FRAMES]['id'].values\n",
    "            printm(f'### Total ids before filtering: {df_id_start_stop.shape[0]}')\n",
    "            df_id_start_stop = df_id_start_stop[df_id_start_stop.total_idxs>MIN_ID_FRAMES].reset_index(drop=True)\n",
    "            printm(f'### Total ids after filtering: {df_id_start_stop.shape[0]}')\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            printm(f'## Map ids into one based on bbox overlap and id start/stop distance')\n",
    "            MAX_ID_DISTANCE = 900\n",
    "            MAX_BBOX_OVERLAP = 0.4\n",
    "            \n",
    "            potential_id_maps = {}\n",
    "            num_possible_maps = 0 \n",
    "            for row_idx, row in df_id_start_stop.iterrows():\n",
    "                row_maxidx = row['max_idx']\n",
    "                \n",
    "                # get polygon for given id\n",
    "                id_max_frame = row_maxidx\n",
    "                id_frame_data = pickle.load(open(f\"{session_frame_dir}/{id_max_frame}.pb\",\"rb\"))[1]\n",
    "                id_frame_data = [xr for xr in id_frame_data if (xr['track_id']==row['id'])][0]\n",
    "                id_bb = id_frame_data['bbox'][:4].astype(int)\n",
    "                X_TL1, Y_TL1, X_BR1, Y_BR1 = id_bb\n",
    "                p1, p2, p3, p4  = map(Point, [[X_TL1,Y_TL1], [X_TL1,Y_BR1], [X_BR1,Y_BR1],[X_BR1, Y_TL1]]) \n",
    "                id_polygon = Polygon(p1, p2, p3, p4)\n",
    "                \n",
    "                potential_id_matches = df_id_start_stop[(df_id_start_stop.min_idx<=row_maxidx+MAX_ID_DISTANCE) & (df_id_start_stop.min_idx>row_maxidx-MAX_ID_DISTANCE)].id.values\n",
    "                successful_matches = []\n",
    "                \n",
    "                if len(potential_id_matches)>0:\n",
    "                    num_possible_maps+=1\n",
    "                    # print('\\n',row['id'], potential_id_matches, row['min_idx'],row['max_idx'],row['total_idxs'])\n",
    "                    for matched_id in potential_id_matches:\n",
    "                        matched_id_min_frame = df_id_start_stop[df_id_start_stop.id==matched_id].min_idx.values[0]\n",
    "                        matched_id_frame_data = pickle.load(open(f\"{session_frame_dir}/{matched_id_min_frame}.pb\",\"rb\"))[1]\n",
    "                        matched_id_frame_data = [xr for xr in matched_id_frame_data if (xr['track_id']==matched_id)][0]\n",
    "                        matched_id_bb = matched_id_frame_data['bbox'][:4].astype(int)\n",
    "                        X_TL2, Y_TL2, X_BR2, Y_BR2 = matched_id_bb\n",
    "                        \n",
    "                        p1, p2, p3, p4  = map(Point, [[X_TL2,Y_TL2], [X_TL2,Y_BR2], [X_BR2,Y_BR2],[X_BR2, Y_TL2]]) \n",
    "                        matched_id_polygon = Polygon(p1, p2, p3, p4)\n",
    "                        \n",
    "                        #find intersection of two polygons\n",
    "                        # check if intersection exists\n",
    "                        if id_polygon.encloses_point(matched_id_polygon.centroid) & matched_id_polygon.encloses_point(id_polygon.centroid):\n",
    "                            X_TL_in, X_BR_in = sorted([X_TL1,X_TL2, X_BR1, X_BR2])[1:3]\n",
    "                            Y_TL_in, Y_BR_in = sorted([Y_TL1,Y_TL2, Y_BR1, Y_BR2])[1:3]\n",
    "                            p1, p2, p3, p4  = map(Point, [[X_TL_in,Y_TL_in], [X_TL_in,Y_BR_in], [X_BR_in,Y_BR_in],[X_BR_in, Y_TL_in]]) \n",
    "                            intersection = Polygon(p1, p2, p3, p4)            \n",
    "            \n",
    "                            #find polygon overlap\n",
    "                            area_intersection = np.abs(intersection.area)\n",
    "                            area_union = np.abs(id_polygon.area) + np.abs(matched_id_polygon.area) - area_intersection\n",
    "                            overlap_fraction  = (area_intersection/area_union).evalf()\n",
    "                        else:\n",
    "                            overlap_fraction=0.            \n",
    "                        if overlap_fraction > MAX_BBOX_OVERLAP:\n",
    "                            successful_matches.append((matched_id, overlap_fraction))\n",
    "                        \n",
    "                        # print('\\tMatching Id: ', matched_id,':', 'frame:',matched_id_min_frame,'overlap_fraction:', overlap_fraction)\n",
    "                if len(successful_matches) > 0:\n",
    "                    successful_matched_id = sorted(successful_matches, key=lambda x: x[1])[-1][0]\n",
    "                    # print(row['id'], '-->Successful match to-->',successful_matched_id)\n",
    "                    if row['id'] in potential_id_maps.keys():\n",
    "                        potential_id_maps[successful_matched_id] = potential_id_maps[row['id']]\n",
    "                    else:\n",
    "                        potential_id_maps[successful_matched_id] = row['id']\n",
    "            \n",
    "            matched_ids  = list(potential_id_maps.keys())\n",
    "            df_id_start_stop =  df_id_start_stop[~df_id_start_stop['id'].isin(matched_ids)].sort_values(by='id').reset_index(drop=True)\n",
    "            printm(f'### Total ids after mapping: {df_id_start_stop.shape[0]}')\n",
    "            \n",
    "            printm(f'## Assign new ids to final set of postprocessed ids')\n",
    "            new_to_old_id_map = df_id_start_stop['id'].to_dict()\n",
    "            old_to_new_id_map = {v: k for k, v in new_to_old_id_map.items()}\n",
    "            \n",
    "            for matched_id in matched_ids:\n",
    "                old_to_new_id_map[matched_id] = old_to_new_id_map[potential_id_maps[matched_id]]\n",
    "            \n",
    "            for removed_id in nonpersistent_ids_removed:\n",
    "                old_to_new_id_map[removed_id] = 10000\n",
    "            \n",
    "            pickle.dump(old_to_new_id_map, open(session_preprocessed_id_map_file,\"wb\"))\n",
    "            printm(f\"## Created ID Map for session: {course_idx}-{course}, session:{session_idx}-{session_id} from {len(old_to_new_id_map.keys())} to {df_id_start_stop.shape[0]} ids\")\n",
    "        else:\n",
    "            printm(f\"## FILE EXISTS: ID Map for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b868a4-bed1-4ff4-9fb2-5b776f5eeba1",
   "metadata": {},
   "source": [
    "# Create Visualization from each session to collect id ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cceb498-d9d4-40fd-8f37-afdd603ecd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_course = '05748A'\n",
    "sample_session_id = 'classinsight-cmu_05748A_ghc_4101_201902141630-front'\n",
    "session_tracking_cache_file = f\"{track_analysis_session_data}/{sample_session_id}.pb\"\n",
    "session_preprocessed_id_map_file = f\"{postprocessed_id_map_data_dir}/{sample_session_id}.pb\"\n",
    "session_frame_dir = f'{base_dir}/{sample_course}/{sample_session_id}'\n",
    "session_video_file = f'/mnt/ci-nas-classes/classinsight/2019S/video_backup/{sample_session_id.split(\"-front\")[0]}/{sample_session_id}.avi'\n",
    "session_frame_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ea750-f727-4e12-b43e-4b2c8c90988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracking_new = pickle.load(open(session_tracking_cache_file,\"rb\")).transpose()\n",
    "old_to_new_id_map = pickle.load(open(session_preprocessed_id_map_file,\"rb\"))\n",
    "total_idxs = df_tracking_new.index.max()\n",
    "for old_id in old_to_new_id_map:\n",
    "    new_id = old_to_new_id_map[old_id]\n",
    "    if not new_id==10000:\n",
    "        new_id_col = f'N{new_id}'\n",
    "        if new_id_col not in df_tracking_new:\n",
    "            df_tracking_new[new_id_col] = None\n",
    "        df_tracking_new[new_id_col] =  df_tracking_new[old_id].where(~df_tracking_new[old_id].isnull(), df_tracking_new[old_id])\n",
    "    df_tracking_new = df_tracking_new.drop(old_id, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2c11b-ce74-408e-8fd6-a4d49f30e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracking_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4832bc-e694-4b5e-a4be-65f3598fe007",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a8a6d-c934-40b3-a712-e7c2e3879010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_to_new_id_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd8987-5e21-4a63-93ad-da3b7e1556df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_start_stop_idxs = []\n",
    "for col in df_tracking_new.columns:\n",
    "    one_idxs = df_tracking_new.index[np.where(df_tracking_new[col]==1)[0]].values\n",
    "    col_start_stop_idxs.append([col, one_idxs.min(), one_idxs.max()])\n",
    "df_id_start_stop = pd.DataFrame(col_start_stop_idxs, columns=['id','min_idx','max_idx'])\n",
    "df_id_start_stop['total_idxs'] = df_id_start_stop['max_idx']-df_id_start_stop['min_idx']\n",
    "df_id_start_stop['id'] = df_id_start_stop['id'].apply(lambda x: int(x[1:]))\n",
    "df_id_start_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837a3d1-1c7c-40b1-905a-13835c22b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(20,15))\n",
    "for row_idx, row in df_id_start_stop.iterrows():\n",
    "    plt.axhline(y=row_idx, xmin=row['min_idx']/total_idxs,xmax=row['max_idx']/total_idxs)\n",
    "plt.yticks(range(df_id_start_stop.shape[0]), range(df_id_start_stop.shape[0]))\n",
    "plt.grid() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258326ec-f91d-4685-a622-73fd28f10161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session_frames = df_tracking_new.index.values.tolist()\n",
    "session_ids_covered = []\n",
    "selected_frames = []\n",
    "segments = []\n",
    "segment_half_size = 50\n",
    "for row_idx, row in df_id_start_stop.sort_values(by='total_idxs').iterrows():\n",
    "    id_start, id_stop = row['min_idx'],row['max_idx']\n",
    "    seg_mid = (id_start+id_stop)/2\n",
    "    seg_start, seg_end = max(seg_mid-segment_half_size, 0), min(seg_mid+segment_half_size, total_idxs)\n",
    "    is_id_covered = df_id_start_stop.apply(lambda row: (seg_end<=row['max_idx']) & (seg_start>=row['min_idx']),axis=1)\n",
    "    seg_ids_covered = df_id_start_stop[is_id_covered]['id']\n",
    "    new_ids_covered = [xr for xr in seg_ids_covered if xr not in session_ids_covered]\n",
    "    if len(new_ids_covered)>0:        \n",
    "        segment_frames = [xr for xr in session_frames if ((xr>=seg_start) and (xr<=seg_end))]\n",
    "        selected_frames+=segment_frames\n",
    "        segments.append((seg_start, seg_end, new_ids_covered, segment_frames))\n",
    "        session_ids_covered+=new_ids_covered\n",
    "\n",
    "sprint(segments)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d57418-1864-4e35-809b-da2421820246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for given frame ranges get video frames and tracking results\n",
    "all_frames = mmcv.VideoReader(session_video_file)\n",
    "\n",
    "required_frame_ids = np.unique(sorted(selected_frames))\n",
    "frame_data_dict = {}\n",
    "\n",
    "for frame_idx, frame_img in enumerate(all_frames):\n",
    "    if (frame_idx in required_frame_ids) & (frame_idx%3==0):\n",
    "        frame_data_dict[frame_idx] = frame_img\n",
    "    if frame_idx%10000==0:\n",
    "        print(f\"Looped {frame_idx} images\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae4995-3301-4ce1-803a-b6873775c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_ROOT = '/home/prasoon/video_analysis/edusenseV2compute/compute/videoV3'\n",
    "run_config = {\n",
    "    'track_config':f'{SOURCE_ROOT}/configs/mmlab/ocsort_yolox_x_crowdhuman_mot17-private-half.py',\n",
    "    'track_checkpoint':f'{SOURCE_ROOT}/models/mmlab/ocsort_yolox_x_crowdhuman_mot17-private-half_20220813_101618-fe150582.pth',\n",
    "    'device':'cuda:1',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe4935-5078-41ca-b37a-29bfe7ab8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frame_data_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9786394-0bf9-4640-86ba-4fdedd2d9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_viz_dir = f'{id_viz_cache_root}/{sample_session_id}'\n",
    "os.makedirs(session_id_viz_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1cde26-0a73-4660-86a8-4c0a6df4cb89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if tracking_model:\n",
    "    printm(\"# Deleting Previous Model\")\n",
    "    del tracking_model\n",
    "tracking_model = init_tracking_model(run_config['track_config'],\n",
    "                                 run_config['track_checkpoint'],\n",
    "                                 device=run_config['device'])\n",
    "for seg_idx, (seg_start, seg_end, new_ids_covered, segment_frames) in enumerate(segments):\n",
    "    seg_video_id = f'S{seg_idx}_{int(seg_start)}_{int(seg_end)}_{\"_\".join(map(str,new_ids_covered))}'\n",
    "    match_cache_dir = f'{session_id_viz_dir}/{seg_video_id}'\n",
    "    match_video_file = f'{session_id_viz_dir}/{seg_video_id}.mp4'\n",
    "    if os.path.exists(match_video_file):\n",
    "        continue\n",
    "    print(match_video_file)\n",
    "    os.makedirs(match_cache_dir, exist_ok=True)\n",
    "    frame_num=0\n",
    "\n",
    "    for frame_idx in segment_frames:\n",
    "        frame_tracking_file = f\"{session_frame_dir}/{frame_idx}.pb\"\n",
    "        if os.path.exists(frame_tracking_file) and (frame_idx in frame_data_dict):\n",
    "            frame_file_out = os.path.join(match_cache_dir, f'{frame_num:06d}.jpg')\n",
    "            frame_num+=1\n",
    "            if os.path.exists(frame_file_out):\n",
    "                continue\n",
    "            frame_tracking_results = pickle.load(open(frame_tracking_file,\"rb\"))[1]\n",
    "            for person_idx in range(len(frame_tracking_results)):\n",
    "                old_id= int(frame_tracking_results[person_idx]['track_id'])\n",
    "                frame_tracking_results[person_idx]['track_id'] =old_to_new_id_map[old_id]\n",
    "            frame_img = deepcopy(frame_data_dict[frame_idx])\n",
    "            if len(frame_tracking_results)>0:\n",
    "                frame_tracking_results = {\n",
    "                    'track_bboxes':[np.array([[xr['track_id']]+xr['bbox'].tolist() for xr in frame_tracking_results])],\n",
    "                    'det_bboxes':[np.array([xr['bbox'].tolist() for xr in frame_tracking_results])]}\n",
    "                frame_track_img = tracking_model.show_result(\n",
    "                    frame_img,\n",
    "                    frame_tracking_results,\n",
    "                    thickness=5,\n",
    "                    font_scale=.5,\n",
    "                    score_thr=0.1,\n",
    "                    show=False,\n",
    "                    wait_time=int(1000. / 5),\n",
    "                    out_file=frame_file_out,\n",
    "                    backend='cv2')\n",
    "            else:\n",
    "                cv2.imwrite(frame_file_out,frame_img)\n",
    "    \n",
    "    #         break\n",
    "    # break\n",
    "    mmcv.frames2video(match_cache_dir, match_video_file, fps=5, fourcc='mp4v')\n",
    "    print(f\"Done for segment {seg_video_id}\")\n",
    "    # break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd4477-a054-4342-9611-4df6e66e6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_viz_cache_root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a398454-cb03-480a-9d8a-2f151a034554",
   "metadata": {},
   "source": [
    "# Run visualization code on all sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50dbb6-3652-4358-b63a-2bcd727dcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_ROOT = '/home/prasoon/video_analysis/edusenseV2compute/compute/videoV3'\n",
    "run_config = {\n",
    "    'track_config':f'{SOURCE_ROOT}/configs/mmlab/ocsort_yolox_x_crowdhuman_mot17-private-half.py',\n",
    "    'track_checkpoint':f'{SOURCE_ROOT}/models/mmlab/ocsort_yolox_x_crowdhuman_mot17-private-half_20220813_101618-fe150582.pth',\n",
    "    'device':'cuda:1',\n",
    "}\n",
    "\n",
    "for course_idx, course in enumerate(frame_file_data):\n",
    "    for session_idx, session_id in enumerate(frame_file_data[course]):\n",
    "        session_id_viz_dir = f'{id_viz_cache_root}/{session_id}'\n",
    "        if not os.path.exists(session_id_viz_dir):\n",
    "            \n",
    "            session_tracking_cache_file = f\"{track_analysis_session_data}/{session_id}.pb\"\n",
    "            session_preprocessed_id_map_file = f\"{postprocessed_id_map_data_dir}/{session_id}.pb\"\n",
    "            session_frame_dir = f'/mnt/ci-nas-cache/edulyzeV2/pose_face_gaze_emb/{course}/{session_id}'\n",
    "            session_video_file = f'/mnt/ci-nas-classes/classinsight/2019S/video_backup/{session_id.split(\"-front\")[0]}/{session_id}.avi'\n",
    "            printm(f'# {course_idx}-{course}, session:{session_idx}-{session_id}')\n",
    "            printm(\"## Get preprocessed tracking ids...\")\n",
    "            \n",
    "            df_tracking_new = pickle.load(open(session_tracking_cache_file,\"rb\")).transpose()\n",
    "            old_to_new_id_map = pickle.load(open(session_preprocessed_id_map_file,\"rb\"))\n",
    "            total_idxs = df_tracking_new.index.max()\n",
    "            for old_id in old_to_new_id_map:\n",
    "                new_id = old_to_new_id_map[old_id]\n",
    "                if not new_id==10000:\n",
    "                    new_id_col = f'N{new_id}'\n",
    "                    if new_id_col not in df_tracking_new:\n",
    "                        df_tracking_new[new_id_col] = None\n",
    "                    df_tracking_new[new_id_col] =  df_tracking_new[old_id].where(~df_tracking_new[old_id].isnull(), df_tracking_new[old_id])\n",
    "                df_tracking_new = df_tracking_new.drop(old_id, axis=1)\n",
    "\n",
    "            col_start_stop_idxs = []\n",
    "            for col in df_tracking_new.columns:\n",
    "                one_idxs = df_tracking_new.index[np.where(df_tracking_new[col]==1)[0]].values\n",
    "                col_start_stop_idxs.append([col, one_idxs.min(), one_idxs.max()])\n",
    "            df_id_start_stop = pd.DataFrame(col_start_stop_idxs, columns=['id','min_idx','max_idx'])\n",
    "            df_id_start_stop['total_idxs'] = df_id_start_stop['max_idx']-df_id_start_stop['min_idx']\n",
    "            df_id_start_stop['id'] = df_id_start_stop['id'].apply(lambda x: int(x[1:]))\n",
    "            \n",
    "            printm(f\"## Get segments to visualize from {df_tracking_new.shape[1]} tracking ids...\")\n",
    "            session_frames = df_tracking_new.index.values.tolist()\n",
    "            session_ids_covered = []\n",
    "            selected_frames = []\n",
    "            segments = []\n",
    "            segment_half_size = 50\n",
    "            for row_idx, row in df_id_start_stop.sort_values(by='total_idxs').iterrows():\n",
    "                id_start, id_stop = row['min_idx'],row['max_idx']\n",
    "                seg_mid = (id_start+id_stop)/2\n",
    "                seg_start, seg_end = max(seg_mid-segment_half_size, 0), min(seg_mid+segment_half_size, total_idxs)\n",
    "                is_id_covered = df_id_start_stop.apply(lambda row: (seg_end<=row['max_idx']) & (seg_start>=row['min_idx']),axis=1)\n",
    "                seg_ids_covered = df_id_start_stop[is_id_covered]['id']\n",
    "                new_ids_covered = [xr for xr in seg_ids_covered if xr not in session_ids_covered]\n",
    "                if len(new_ids_covered)>0:        \n",
    "                    segment_frames = [xr for xr in session_frames if ((xr>=seg_start) and (xr<=seg_end))]\n",
    "                    selected_frames+=segment_frames\n",
    "                    segments.append((seg_start, seg_end, new_ids_covered, segment_frames))\n",
    "                    session_ids_covered+=new_ids_covered\n",
    "            printm(f\"## Got {len(segments)} segments to visualize...\")\n",
    "\n",
    "            printm(f\"## Get video frames from session video file for segment frames...\")\n",
    "            all_frames = mmcv.VideoReader(session_video_file)\n",
    "            required_frame_ids = np.unique(sorted(selected_frames))\n",
    "            frame_data_dict = dict()\n",
    "            \n",
    "            for frame_idx, frame_img in enumerate(all_frames):\n",
    "                if (frame_idx in required_frame_ids) & (frame_idx%3==0):\n",
    "                    frame_data_dict[frame_idx] = frame_img\n",
    "                if frame_idx%1000==0:\n",
    "                    print(f\"Looped {frame_idx} images\")\n",
    "            printm(f\"## Got {len(frame_data_dict.keys())} video frames from session video file...\")\n",
    "\n",
    "            printm(f\"## Create segment videos with new tracking ids...\")\n",
    "            os.makedirs(session_id_viz_dir, exist_ok=True)\n",
    "            tracking_model = init_tracking_model(run_config['track_config'],\n",
    "                                             run_config['track_checkpoint'],\n",
    "                                             device=run_config['device'])\n",
    "            for seg_idx, (seg_start, seg_end, new_ids_covered, segment_frames) in enumerate(segments):\n",
    "                seg_video_id = f'S{seg_idx}_{int(seg_start)}_{int(seg_end)}_{\"_\".join(map(str,new_ids_covered))}'\n",
    "                match_cache_dir = f'{session_id_viz_dir}/{seg_video_id}'\n",
    "                match_video_file = f'{session_id_viz_dir}/{seg_video_id}.mp4'\n",
    "                if os.path.exists(match_video_file):\n",
    "                    continue\n",
    "                print(match_video_file)\n",
    "                os.makedirs(match_cache_dir, exist_ok=True)\n",
    "                frame_num=0\n",
    "            \n",
    "                for frame_idx in segment_frames:\n",
    "                    frame_tracking_file = f\"{session_frame_dir}/{frame_idx}.pb\"\n",
    "                    if os.path.exists(frame_tracking_file) and (frame_idx in frame_data_dict):\n",
    "                        frame_file_out = os.path.join(match_cache_dir, f'{frame_num:06d}.jpg')\n",
    "                        frame_num+=1\n",
    "                        if os.path.exists(frame_file_out):\n",
    "                            continue\n",
    "                        frame_tracking_results = pickle.load(open(frame_tracking_file,\"rb\"))[1]\n",
    "                        for person_idx in range(len(frame_tracking_results)):\n",
    "                            old_id= int(frame_tracking_results[person_idx]['track_id'])\n",
    "                            frame_tracking_results[person_idx]['track_id'] =old_to_new_id_map[old_id]\n",
    "                        frame_img = deepcopy(frame_data_dict[frame_idx])\n",
    "                        if len(frame_tracking_results)>0:\n",
    "                            frame_tracking_results = {\n",
    "                                'track_bboxes':[np.array([[xr['track_id']]+xr['bbox'].tolist() for xr in frame_tracking_results])],\n",
    "                                'det_bboxes':[np.array([xr['bbox'].tolist() for xr in frame_tracking_results])]}\n",
    "                            frame_track_img = tracking_model.show_result(\n",
    "                                frame_img,\n",
    "                                frame_tracking_results,\n",
    "                                thickness=5,\n",
    "                                font_scale=.5,\n",
    "                                score_thr=0.1,\n",
    "                                show=False,\n",
    "                                wait_time=int(1000. / 5),\n",
    "                                out_file=frame_file_out,\n",
    "                                backend='cv2')\n",
    "                        else:\n",
    "                            cv2.imwrite(frame_file_out,frame_img)\n",
    "                mmcv.frames2video(match_cache_dir, match_video_file, fps=5, fourcc='mp4v')\n",
    "                print(f\"Done for segment {seg_video_id}\")\n",
    "            del tracking_model\n",
    "            printm(f\"## Created Segment Visualization for session: {course_idx}-{course}, session:{session_idx}-{session_id} from {len(old_to_new_id_map.keys())} to {df_id_start_stop.shape[0]} ids\")\n",
    "        else:\n",
    "            printm(f\"## Segement visualization directory EXISTS for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a688fb-6c62-4f33-9bd4-616d70db613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_video_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0eea6-acad-433b-829c-7de6fd98e213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
