{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1bf94-25d4-4ed1-bddd-bc3309763692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "# import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\", context=\"paper\")\n",
    "from cycler import cycler\n",
    "import os, sys\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import combinations, product\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO as _BytesIO\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.metrics import *\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "import traceback\n",
    "from sympy import Point, Polygon\n",
    "from decorators import *\n",
    "from smartprint import smartprint as sprint\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import DBSCAN\n",
    "# import plotly\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "def printm(s): return display(Markdown(s))\n",
    "    \n",
    "SERVER_CACHE_DIR = '/mnt/ci-nas-cache/edulyzeV2/cache_compute_4/fixed_face'\n",
    "os.makedirs(SERVER_CACHE_DIR,exist_ok=True)\n",
    "\n",
    "track_analysis_meta_cache = f'{SERVER_CACHE_DIR}/analysis_tracking/meta_info'\n",
    "base_dir = '/mnt/ci-nas-cache/edulyzeV2/pose_face_gaze_emb_fixed_face/'\n",
    "\n",
    "track_analysis_session_data = f'{SERVER_CACHE_DIR}/analysis_tracking/session_tracking_info'\n",
    "os.makedirs(track_analysis_session_data,exist_ok=True)\n",
    "\n",
    "postprocessed_id_map_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/processed_id_maps'\n",
    "os.makedirs(postprocessed_id_map_data_dir, exist_ok=True)\n",
    "\n",
    "emb_analysis_session_data = f'{SERVER_CACHE_DIR}/analysis_emb/session_emb_info_new'\n",
    "os.makedirs(emb_analysis_session_data,exist_ok=True)\n",
    "\n",
    "embmatched_id_raw_data_dir = f'{SERVER_CACHE_DIR}/analysis_emb/embmatched_id_raw'\n",
    "os.makedirs(embmatched_id_raw_data_dir,exist_ok=True)\n",
    "\n",
    "embmatched_id_map_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/embmatched_id_maps_new'\n",
    "os.makedirs(embmatched_id_map_data_dir, exist_ok=True)\n",
    "\n",
    "in_session_median_embeddings_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/in_session_median_embeddings'\n",
    "os.makedirs(in_session_median_embeddings_data_dir, exist_ok=True)\n",
    "\n",
    "in_session_cluster_embeddings_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/in_session_cluster_embeddings'\n",
    "os.makedirs(in_session_cluster_embeddings_data_dir, exist_ok=True)\n",
    "\n",
    "id_viz_cache_root = f'{SERVER_CACHE_DIR}/analysis_tracking/session_matching_info'\n",
    "os.makedirs(id_viz_cache_root, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565327ad-9d75-4198-a740-74e64061e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_filter_list = [\n",
    "   'classinsight-cmu_05681A_ghc_4301_201905011630',\n",
    " 'classinsight-cmu_05681A_ghc_4301_201904171630',\n",
    " 'classinsight-cmu_05681A_ghc_4301_201902201630',\n",
    " 'classinsight-cmu_05681A_ghc_4301_201904101630',\n",
    " 'classinsight-cmu_05681A_ghc_4301_201901231630',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201902251200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904081200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201905011200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904291200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904011200',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201902141630',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201904021630',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201902051630',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201902281630',\n",
    " # 'classinsight-cmu_05748A_ghc_4101_201903071630',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904230930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201903260930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904160930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904300930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201903190930',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904151500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201902251500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904081500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904221500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201902181500',\n",
    "                       \n",
    " # 'classinsight-cmu_17214B_ph_a21_201902271030',\n",
    " # 'classinsight-cmu_17214B_ph_a21_201903061030',\n",
    " # 'classinsight-cmu_17214B_ph_a21_201904031030',\n",
    " # 'classinsight-cmu_17214B_ph_a21_201904101030',\n",
    " # 'classinsight-cmu_17214B_ph_a21_201904241030',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201903201130',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201904101130',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201904171130',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201904241130',\n",
    " # 'classinsight-cmu_17214C_ph_225b_201905011130',\n",
    " 'classinsight-cmu_05410B_ghc_4211_201902111500',\n",
    " 'classinsight-cmu_05410B_ghc_4211_201903181500',\n",
    " 'classinsight-cmu_05410B_ghc_4211_201904081500',\n",
    " 'classinsight-cmu_05410B_ghc_4211_201904151500',\n",
    " 'classinsight-cmu_05410B_ghc_4211_201904221500',\n",
    " 'classinsight-cmu_05410B_ghc_4211_201901281500'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6410b40-d069-4aab-9000-36e08b0a0763",
   "metadata": {},
   "source": [
    "## Get embedding and gaze information for all frames for all sessions (Run if needed, commented out for now)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75185159-ce0e-439f-b41d-fba37829e865",
   "metadata": {},
   "source": [
    "## Get frame file data for all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61657d24-cddc-4722-bbde-b99313f4c2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "frame_file_data = {}\n",
    "for course_idx, course_dir in enumerate(glob.glob(f\"{base_dir}/*\")):\n",
    "    course_name = course_dir.split(\"/\")[-1]\n",
    "    course_cache_file = f\"{track_analysis_meta_cache}/{course_name}\"\n",
    "    if os.path.exists(course_cache_file):\n",
    "        frame_file_data[course_name] = pickle.load(open(course_cache_file,\"rb\"))\n",
    "        continue\n",
    "    frame_file_data[course_name]={}\n",
    "        \n",
    "    for session_idx, session_dir in enumerate(glob.glob(f\"{course_dir}/*\")):\n",
    "        session_name = session_dir.split(\"/\")[-1]\n",
    "        frame_file_data[course_name][session_name] = {}\n",
    "        frame_files = glob.glob(f\"{session_dir}/*\")\n",
    "        frame_file_names = [xr.split(\"/\")[-1] for xr in frame_files]\n",
    "        if 'end.pb' in frame_file_names:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=True\n",
    "        else:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=False            \n",
    "        frame_ids = [int(xr.split(\".\")[0]) for xr in frame_file_names if not (xr=='end.pb')]\n",
    "        frame_file_data[course_name][session_name]['frame_ids'] = sorted(frame_ids)\n",
    "        frame_file_data[course_name][session_name]['dir_location'] = session_dir\n",
    "        print(f\"Got metadata for course: {course_idx}-{course_name}, session:{session_idx}-{session_name}\")\n",
    "    pickle.dump(frame_file_data[course_name],open(course_cache_file,\"wb\")) \n",
    "        \n",
    "        \n",
    "frame_file_data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d3c5e-7b9d-4919-888b-26526be404f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writing a generic loop to get embedding info from all courses in frame file data\n",
    "\n",
    "for course_idx, course in enumerate(frame_file_data):\n",
    "    for session_idx, session_id in enumerate(frame_file_data[course]):\n",
    "        if session_id.split(\"-front\")[0] not in session_filter_list:\n",
    "            print(f\"Session {session_id} not in session filter list, skipping...\")\n",
    "            continue\n",
    "        session_emb_cache_file = f\"{emb_analysis_session_data}/{session_id}.pb\"\n",
    "        try:\n",
    "            if not os.path.exists(session_emb_cache_file):\n",
    "                session_dir = frame_file_data[course][session_id]['dir_location']\n",
    "                frame_ids = frame_file_data[course][session_id]['frame_ids']\n",
    "                session_emb_info = {}\n",
    "                for frame_id in frame_ids:\n",
    "                    frame_number, frame_data = pickle.load(open(f'{session_dir}/{frame_id}.pb','rb'))\n",
    "                    frame_emb_info = {int(person_info['track_id']):{\n",
    "                        'bbox': person_info['bbox'] if 'bbox' in person_info else None,\n",
    "                        'rvec': person_info['rvec'] if 'rvec' in person_info else None,\n",
    "                        'face': person_info['face'] if 'face' in person_info else None,\n",
    "                        'gaze_2d':person_info['gaze_2d'] if 'gaze_2d' in person_info else None,\n",
    "                        'face_embedding': person_info['face_embedding'] if 'face_embedding' in person_info else None,\n",
    "                    } for person_info in frame_data}\n",
    "                    session_emb_info[frame_id] = frame_emb_info\n",
    "                pickle.dump(session_emb_info, open(session_emb_cache_file,'wb'))\n",
    "                print(f\"Got emb info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            else:\n",
    "                ...\n",
    "                print(f\"FILE EXISTS: emb info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "        except:\n",
    "            print(f\"ERROR: Unable to get session emb for: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            unfinished_sessions.append((course, session_id))\n",
    "            print(traceback.format_exc())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482ca00-a3a2-41b9-a223-cc62bfed87c3",
   "metadata": {},
   "source": [
    "## Compile information from session embeddings and postprocessed id maps to get in-session id maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a061b5-bf4c-4e3c-9de8-67269566c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlapping_metric(bu, bv, eps_fraction=0.1):\n",
    "    X_TL1, Y_TL1, X_BR1, Y_BR1 = bu[:4]\n",
    "    X_TL2, Y_TL2, X_BR2, Y_BR2 = bv[:4]\n",
    "    eps_distance = min(X_BR1-X_TL1, X_BR2-X_TL2, Y_BR1-Y_TL1, Y_BR2-Y_TL2)*eps_fraction\n",
    "    # if rectangle has area 0, no overlap\n",
    "    if X_TL1 == X_BR1 or Y_TL1 == Y_BR1 or X_TL2 == X_BR2 or Y_TL2 == Y_BR2:\n",
    "        return False\n",
    "     \n",
    "    # If one rectangle is on left side of other\n",
    "    if X_TL1 > X_BR2-eps_distance or X_TL2 > X_BR1-eps_distance:\n",
    "        return False\n",
    " \n",
    "    # If one rectangle is above other\n",
    "    if Y_TL1 > Y_BR2 - eps_distance or Y_TL2 > Y_BR1 - eps_distance:\n",
    "        return False\n",
    " \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0da6f-ca81-4092-ae48-103ecc7fe3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CLU_EPS = 0.4\n",
    "CLU_MIN_PTS = 100\n",
    "MATCH_DISTANCE_THRESHOLD = 0.2\n",
    "BBOX_OVERLAP_THRESHOLD = 0.8\n",
    "\n",
    "for course_idx, course in enumerate(frame_file_data):\n",
    "    for session_idx, session_id in enumerate(frame_file_data[course]):\n",
    "        if session_id.split(\"-front\")[0] not in session_filter_list:\n",
    "            print(f\"Session {session_id} not in session filter list, skipping...\")\n",
    "            continue\n",
    "        embmatch_map_cache_file = f\"{embmatched_id_map_data_dir}/{session_id}.csv\"\n",
    "        try:\n",
    "            if not os.path.exists(embmatch_map_cache_file):\n",
    "                session_emb_info = pickle.load(open(f'{emb_analysis_session_data}/{session_id}.pb','rb'))\n",
    "                session_id_map = pickle.load(open(f\"{postprocessed_id_map_data_dir}/{session_id}.pb\",\"rb\"))\n",
    "                df_tracking_new = pickle.load(open(f\"{track_analysis_session_data}/{session_id}.pb\",\"rb\")).transpose()\n",
    "        \n",
    "                printm(\"## Replace raw ids with mapped ids for given session\")\n",
    "                session_emb_info = {\n",
    "                    xr:{\n",
    "                        session_id_map[yr]:session_emb_info[xr][yr] \n",
    "                            for yr in session_emb_info[xr] if not (session_id_map[yr]==10000)} for xr in session_emb_info}\n",
    "                \n",
    "                # arrange info as per new tracking id for entire session\n",
    "                printm(\"## arrange info as per new tracking id for entire session\")\n",
    "                gaze_info = {}\n",
    "                emb_info = {}\n",
    "                bbox_info = {}\n",
    "                \n",
    "                for frame_number in session_emb_info:\n",
    "                    for trackId in session_emb_info[frame_number]:\n",
    "                        if trackId not in gaze_info:\n",
    "                            gaze_info[trackId] = []\n",
    "                            emb_info[trackId]=[]\n",
    "                            bbox_info[trackId] = []\n",
    "                        # get  gaze info\n",
    "                        try:\n",
    "                            id_bbox = session_emb_info[frame_number][trackId]['bbox']\n",
    "                            bbox_info[trackId].append([frame_number]+list(id_bbox))\n",
    "                            pitch, roll, yaw= session_emb_info[frame_number][trackId]['rvec'][0]\n",
    "                            pitch, roll, yaw=np.rad2deg(pitch), np.rad2deg(roll), np.rad2deg(yaw)\n",
    "                            gaze_sx, gaze_sy, gaze_ex, gaze_ey = session_emb_info[frame_number][trackId]['gaze_2d'][0].flatten()\n",
    "                            gaze_info[trackId].append([frame_number, pitch, roll, yaw, gaze_sx, gaze_sy, gaze_ex, gaze_ey])\n",
    "                            face_emb = session_emb_info[frame_number][trackId]['face_embedding'].tolist()\n",
    "                            emb_info[trackId].append([frame_number]+face_emb)\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                for id in gaze_info:\n",
    "                    gaze_info[id] = pd.DataFrame(gaze_info[id], columns=['frame','pitch','roll','yaw','gaze_sx', 'gaze_sy', 'gaze_ex', 'gaze_ey']).set_index('frame')\n",
    "                    emb_info[id] =pd.DataFrame(emb_info[id], columns=['frame']+np.arange(512).tolist()).set_index('frame')\n",
    "                    bbox_info[id] = pd.DataFrame(bbox_info[id], columns=['frame']+np.arange(5).tolist()).set_index('frame')\n",
    "                \n",
    "                \n",
    "                # Get id start stop for given session (needed to evaluate overlap conditions)\n",
    "                printm(\"## Get id start stop for given session (needed to evaluate overlap conditions)\")\n",
    "                total_idxs = df_tracking_new.index.max()\n",
    "                for old_id in session_id_map:\n",
    "                    new_id = session_id_map[old_id]\n",
    "                    if not new_id==10000:\n",
    "                        new_id_col = f'N{new_id}'\n",
    "                        if new_id_col not in df_tracking_new:\n",
    "                            df_tracking_new[new_id_col] = None\n",
    "                        df_tracking_new[new_id_col] =  df_tracking_new[new_id_col].where(~df_tracking_new[new_id_col].isnull(), df_tracking_new[old_id])\n",
    "                    df_tracking_new = df_tracking_new.drop(old_id, axis=1)\n",
    "                \n",
    "                col_start_stop_idxs = []\n",
    "                for col in df_tracking_new.columns:\n",
    "                    one_idxs = df_tracking_new.index[np.where(df_tracking_new[col]==1)[0]].values\n",
    "                    col_start_stop_idxs.append([col, one_idxs.min(), one_idxs.max()])\n",
    "                df_id_start_stop = pd.DataFrame(col_start_stop_idxs, columns=['id','min_idx','max_idx'])\n",
    "                df_id_start_stop['total_idxs'] = df_id_start_stop['max_idx']-df_id_start_stop['min_idx']\n",
    "                df_id_start_stop['id'] = df_id_start_stop['id'].apply(lambda x: int(x[1:]))\n",
    "        \n",
    "                # Use spectral clustering to get clean set of embeddings and calculate their centroid\n",
    "                printm(\"## Use spectral clustering to get clean set of embeddings and calculate their centroid\")\n",
    "                np.random.seed(42)\n",
    "                clustered_median_emb = {}\n",
    "                for id in emb_info:\n",
    "                    emb_clu = DBSCAN(min_samples=CLU_MIN_PTS, eps=CLU_EPS)\n",
    "                    try:\n",
    "                        emb_clu.fit(emb_info[id].values)\n",
    "                    except:\n",
    "                        emb_clu=None\n",
    "                    if (emb_clu is None) or (max(emb_clu.labels_)<0):\n",
    "                        sprint(f\"All frames are outliers, not proceeding with id {id}\")\n",
    "                        continue\n",
    "                    best_cluster_id = pd.Series(emb_clu.labels_[emb_clu.labels_>=0]).value_counts().index[0]\n",
    "                    frames = emb_info[id].iloc[emb_clu.labels_==best_cluster_id].index.values\n",
    "                    clustered_median_emb[id] = np.median(emb_info[id].loc[frames],axis=0)\n",
    "        \n",
    "                # Evaluate matching distance for temporally non overlapping ids\n",
    "                printm(\"## Evaluate matching distance for temporally non overlapping ids\")\n",
    "                match_scores = {}\n",
    "                for idA in sorted(clustered_median_emb.keys()):\n",
    "                    for idB in sorted(clustered_median_emb.keys()):\n",
    "                        if idB in match_scores.keys():\n",
    "                            continue\n",
    "                        # check if idA and idB overlaps, if not, Just leave them be\n",
    "                        min_idxA, max_idxA = df_id_start_stop[df_id_start_stop['id']==idA][['min_idx','max_idx']].values[0].tolist()\n",
    "                        min_idxB, max_idxB = df_id_start_stop[df_id_start_stop['id']==idB][['min_idx','max_idx']].values[0].tolist()\n",
    "                        if len(range(max(min_idxA,min_idxB), min(max_idxA,max_idxB))) > 150:#more than 10 seconds of overlap\n",
    "                            #overlapping ranges\n",
    "                            continue\n",
    "                        match_distance = cdist(clustered_median_emb[idA].reshape(1,-1), clustered_median_emb[idB].reshape(1,-1))[0][0]\n",
    "                        if match_distance < MATCH_DISTANCE_THRESHOLD:\n",
    "                            if idA not in match_scores:\n",
    "                                match_scores[idA] = {}\n",
    "                            match_scores[idA][idB] = match_distance\n",
    "                        \n",
    "                df_matching_method = pd.DataFrame(match_scores)\n",
    "                if df_matching_method.shape[0]>0:\n",
    "                    fig, axn = plt.subplots(1,1,figsize=(20,10))\n",
    "                    sns.heatmap(df_matching_method.round(2), annot=True,ax=axn,cmap='bone_r')\n",
    "                    axn.set_xlabel(f\"Session: {session_id}\",fontsize=16)\n",
    "                    axn.set_ylabel(f\"Session: {session_id}\",fontsize=16)\n",
    "                    plt.savefig(f'plots/self_match_{course}_{session_id.split(\"-front\")[0].split(\"_\")[-1]}.png',dpi=400,bbox_inches='tight')\n",
    "                \n",
    "        \n",
    "                # Evaluate bbox overlap to filter out spatially overlapping ids\n",
    "                printm(\"## Evaluate bbox overlap to filter out spatially overlapping ids\")\n",
    "                overlap_scores = {}\n",
    "                for idA in sorted(clustered_median_emb.keys()):\n",
    "                    for idB in sorted(clustered_median_emb.keys()):\n",
    "                        if idB in match_scores.keys():\n",
    "                            continue\n",
    "                        # check if idA and idB overlaps, if not, Just leave them be\n",
    "                        min_idxA, max_idxA = df_id_start_stop[df_id_start_stop['id']==idA][['min_idx','max_idx']].values[0].tolist()\n",
    "                        min_idxB, max_idxB = df_id_start_stop[df_id_start_stop['id']==idB][['min_idx','max_idx']].values[0].tolist()\n",
    "                        if len(range(max(min_idxA,min_idxB), min(max_idxA,max_idxB))) > 0:\n",
    "                            #overlapping ranges\n",
    "                            continue\n",
    "                        bbox_overlap_matrix = cdist(bbox_info[idA].iloc[:1000], bbox_info[idB].iloc[:1000], metric = is_overlapping_metric)\n",
    "                        bbox_overlap = np.mean(bbox_overlap_matrix.flatten())\n",
    "                        sprint(idA, idB, bbox_overlap)\n",
    "                        if bbox_overlap > BBOX_OVERLAP_THRESHOLD:\n",
    "                            if idA not in overlap_scores:\n",
    "                                overlap_scores[idA] = {}\n",
    "                            overlap_scores[idA][idB] = bbox_overlap\n",
    "                        \n",
    "                df_overlap = pd.DataFrame(overlap_scores)\n",
    "\n",
    "                    \n",
    "                if df_overlap.shape[0]>0:\n",
    "                    fig, axn = plt.subplots(1,1,figsize=(20,10))\n",
    "                    sns.heatmap(df_overlap.round(2), annot=True,ax=axn,cmap='bone_r')\n",
    "                    axn.set_xlabel(f\"Session: {session_id}\",fontsize=16)\n",
    "                    axn.set_ylabel(f\"Session: {session_id}\",fontsize=16)\n",
    "                    plt.savefig(f'plots/self_overlap_{course}_{session_id.split(\"-front\")[0].split(\"_\")[-1]}.png',dpi=400,bbox_inches='tight')\n",
    "        \n",
    "                # get eligible pairs from matching and spatial overlap information\n",
    "                printm(\"## get eligible pairs from matching and spatial overlap information\")\n",
    "                if (df_matching_method.shape[0]==0) or (df_overlap.shape[0]==0):\n",
    "                    df_eligible_pairs = pd.DataFrame(columns=[\"id_pair\",\"value_overlap\",\"value_match\"])\n",
    "                else:\n",
    "                    df_overlap_melted = df_overlap.reset_index().melt(id_vars='index')\n",
    "                    df_overlap_melted = df_overlap_melted[~df_overlap_melted['value'].isnull()]\n",
    "                    df_overlap_melted['id_pair'] = df_overlap_melted.apply(lambda row: tuple(sorted([int(row['index']),int(row['variable'])])), axis=1)\n",
    "                    df_overlap_melted = df_overlap_melted[['id_pair','value']]\n",
    "                    df_overlap_melted\n",
    "                    \n",
    "                    df_match_melted = df_matching_method.reset_index().melt(id_vars='index')\n",
    "                    df_match_melted = df_match_melted[~df_match_melted['value'].isnull()]\n",
    "                    df_match_melted['id_pair'] = df_match_melted.apply(lambda row: tuple(sorted([int(row['index']),int(row['variable'])])), axis=1)\n",
    "                    df_match_melted = df_match_melted[['id_pair','value']]\n",
    "                    df_match_melted\n",
    "                    \n",
    "                    df_eligible_pairs = pd.merge(df_overlap_melted, df_match_melted, on='id_pair',suffixes=('_overlap','_match'))\n",
    "                df_eligible_pairs.to_csv(embmatch_map_cache_file,index=False)\n",
    "        \n",
    "                embmatch_raw_data_dict = {\n",
    "                    'overlap_df':df_overlap,\n",
    "                    'match_df':df_matching_method,\n",
    "                    'eligible_pairs_df':df_eligible_pairs,\n",
    "                    'id_session_embeddings':clustered_median_emb,\n",
    "                    'id_start_stop_df':df_id_start_stop\n",
    "                }\n",
    "                pickle.dump(embmatch_raw_data_dict, open(f\"{embmatched_id_raw_data_dir}/{session_id}.pb\",\"wb\"))\n",
    "                printm(f\"## Got embedding based id match for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "                print(f\"{df_eligible_pairs.id_pair.values}\")\n",
    "            else:\n",
    "                print(f\"FILE EXISTS: embedding based id match for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "        except:\n",
    "            printm(f\"# ERROR: Unable to get embedding based id match for: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            print(traceback.format_exc())\n",
    "            time.sleep(5)\n",
    "        printm(f\"# -----------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5148e47-f8a0-4749-9734-3b17446e8f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
