{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1bf94-25d4-4ed1-bddd-bc3309763692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "# import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\", context=\"paper\")\n",
    "from cycler import cycler\n",
    "import os, sys\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import combinations, product\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO as _BytesIO\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.metrics import *\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "import traceback\n",
    "from sympy import Point, Polygon\n",
    "from decorators import *\n",
    "from smartprint import smartprint as sprint\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import DBSCAN\n",
    "import mmcv\n",
    "from mmtrack.apis import inference_mot, init_model as init_tracking_model\n",
    "# import plotly\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "def printm(s): return display(Markdown(s))\n",
    "    \n",
    "SERVER_CACHE_DIR = '/mnt/ci-nas-cache/edulyzeV2/cache_compute_4/fixed_face'\n",
    "os.makedirs(SERVER_CACHE_DIR,exist_ok=True)\n",
    "\n",
    "track_analysis_meta_cache = f'{SERVER_CACHE_DIR}/analysis_tracking/meta_info'\n",
    "base_dir = '/mnt/ci-nas-cache/edulyzeV2/pose_face_gaze_emb_fixed_face/'\n",
    "\n",
    "track_analysis_session_data = f'{SERVER_CACHE_DIR}/analysis_tracking/session_tracking_info'\n",
    "os.makedirs(track_analysis_session_data,exist_ok=True)\n",
    "\n",
    "postprocessed_id_map_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/processed_id_maps'\n",
    "os.makedirs(postprocessed_id_map_data_dir, exist_ok=True)\n",
    "\n",
    "emb_analysis_session_data = f'{SERVER_CACHE_DIR}/analysis_emb/session_emb_info_new'\n",
    "os.makedirs(emb_analysis_session_data,exist_ok=True)\n",
    "\n",
    "embmatched_id_raw_data_dir = f'{SERVER_CACHE_DIR}/analysis_emb/embmatched_id_raw'\n",
    "os.makedirs(embmatched_id_raw_data_dir,exist_ok=True)\n",
    "\n",
    "embmatched_id_map_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/embmatched_id_maps_new'\n",
    "os.makedirs(embmatched_id_map_data_dir, exist_ok=True)\n",
    "\n",
    "cross_session_input_data_dir = f'{SERVER_CACHE_DIR}/analysis_emb/cross_session_input'\n",
    "os.makedirs(cross_session_input_data_dir, exist_ok=True)\n",
    "\n",
    "id_viz_cache_root = f'{SERVER_CACHE_DIR}/analysis_emb/session_matching_info'\n",
    "os.makedirs(id_viz_cache_root, exist_ok=True)\n",
    "\n",
    "v5_id_start_stop_data_dir = f'{SERVER_CACHE_DIR}/analysis_emb/v5_id_start_stop_info'\n",
    "os.makedirs(v5_id_start_stop_data_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75185159-ce0e-439f-b41d-fba37829e865",
   "metadata": {},
   "source": [
    "## Get frame file data for all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61657d24-cddc-4722-bbde-b99313f4c2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "frame_file_data = {}\n",
    "for course_idx, course_dir in enumerate(glob.glob(f\"{base_dir}/*\")):\n",
    "    course_name = course_dir.split(\"/\")[-1]\n",
    "    course_cache_file = f\"{track_analysis_meta_cache}/{course_name}\"\n",
    "    if os.path.exists(course_cache_file):\n",
    "        frame_file_data[course_name] = pickle.load(open(course_cache_file,\"rb\"))\n",
    "        continue\n",
    "    frame_file_data[course_name]={}\n",
    "        \n",
    "    for session_idx, session_dir in enumerate(glob.glob(f\"{course_dir}/*\")):\n",
    "        session_name = session_dir.split(\"/\")[-1]\n",
    "        frame_file_data[course_name][session_name] = {}\n",
    "        frame_files = glob.glob(f\"{session_dir}/*\")\n",
    "        frame_file_names = [xr.split(\"/\")[-1] for xr in frame_files]\n",
    "        if 'end.pb' in frame_file_names:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=True\n",
    "        else:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=False            \n",
    "        frame_ids = [int(xr.split(\".\")[0]) for xr in frame_file_names if not (xr=='end.pb')]\n",
    "        frame_file_data[course_name][session_name]['frame_ids'] = sorted(frame_ids)\n",
    "        frame_file_data[course_name][session_name]['dir_location'] = session_dir\n",
    "        print(f\"Got metadata for course: {course_idx}-{course_name}, session:{session_idx}-{session_name}\")\n",
    "    pickle.dump(frame_file_data[course_name],open(course_cache_file,\"wb\")) \n",
    "        \n",
    "frame_file_data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf998f-dc47-447f-b1a8-14d55d69057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_filter_list = [\n",
    " #    'classinsight-cmu_05681A_ghc_4301_201905011630',\n",
    " # 'classinsight-cmu_05681A_ghc_4301_201904171630',\n",
    " # 'classinsight-cmu_05681A_ghc_4301_201902201630',\n",
    " # 'classinsight-cmu_05681A_ghc_4301_201904101630',\n",
    " # 'classinsight-cmu_05681A_ghc_4301_201901231630',\n",
    "                       \n",
    " # 'classinsight-cmu_05418A_ghc_4102_201902251200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904081200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201905011200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904291200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904011200',\n",
    "                       \n",
    " 'classinsight-cmu_05748A_ghc_4101_201902141630',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201904021630',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201902051630',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201902281630',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201903071630',\n",
    "                       \n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904230930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201903260930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904160930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904300930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201903190930',\n",
    "                       \n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904151500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201902251500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904081500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904221500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201902181500',\n",
    "                       \n",
    " 'classinsight-cmu_17214B_ph_a21_201902271030',\n",
    " 'classinsight-cmu_17214B_ph_a21_201903061030',\n",
    " 'classinsight-cmu_17214B_ph_a21_201904031030',\n",
    " 'classinsight-cmu_17214B_ph_a21_201904101030',\n",
    " 'classinsight-cmu_17214B_ph_a21_201904241030',\n",
    "                       \n",
    " 'classinsight-cmu_17214C_ph_225b_201903201130',\n",
    " 'classinsight-cmu_17214C_ph_225b_201904101130',\n",
    " 'classinsight-cmu_17214C_ph_225b_201904171130',\n",
    " 'classinsight-cmu_17214C_ph_225b_201904241130',\n",
    " 'classinsight-cmu_17214C_ph_225b_201905011130',\n",
    "                       \n",
    " # 'classinsight-cmu_05410B_ghc_4211_201902111500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201903181500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904081500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904151500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904221500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201901281500'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bae7d-c670-434d-b62e-a9ac7b463e32",
   "metadata": {},
   "source": [
    "# Get frames data across all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb86cc3-3f32-4ed4-b556-bf74efde0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_file_data = {}\n",
    "for course_idx, course_dir in enumerate(glob.glob(f\"{base_dir}/*\")):\n",
    "    course_name = course_dir.split(\"/\")[-1]\n",
    "    course_cache_file = f\"{track_analysis_meta_cache}/{course_name}\"\n",
    "    if os.path.exists(course_cache_file):\n",
    "        frame_file_data[course_name] = pickle.load(open(course_cache_file,\"rb\"))\n",
    "        continue\n",
    "    frame_file_data[course_name]={}\n",
    "        \n",
    "    for session_idx, session_dir in enumerate(glob.glob(f\"{course_dir}/*\")):\n",
    "        session_name = session_dir.split(\"/\")[-1]\n",
    "        frame_file_data[course_name][session_name] = {}\n",
    "        frame_files = glob.glob(f\"{session_dir}/*\")\n",
    "        frame_file_names = [xr.split(\"/\")[-1] for xr in frame_files]\n",
    "        if 'end.pb' in frame_file_names:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=True\n",
    "        else:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=False            \n",
    "        frame_ids = [int(xr.split(\".\")[0]) for xr in frame_file_names if not (xr=='end.pb')]\n",
    "        frame_file_data[course_name][session_name]['frame_ids'] = sorted(frame_ids)\n",
    "        frame_file_data[course_name][session_name]['dir_location'] = session_dir\n",
    "        print(f\"Got metadata for course: {course_idx}-{course_name}, session:{session_idx}-{session_name}\")\n",
    "    pickle.dump(frame_file_data[course_name],open(course_cache_file,\"wb\")) \n",
    "        \n",
    "frame_file_data.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b5624-5d79-48d7-a77e-b96a53978c1c",
   "metadata": {},
   "source": [
    "# Get id_start_stop info for all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d712af-3fe7-4f3b-a71a-d0d022b737bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writing a generic loop to get embedding info from all courses in frame file data\n",
    "\n",
    "for course_idx, course in enumerate(frame_file_data):\n",
    "    for session_idx, session_id in enumerate(frame_file_data[course]):\n",
    "        if session_id.split(\"-front\")[0] not in session_filter_list:\n",
    "            print(f\"Session {session_id} not in session filter list, skipping...\")\n",
    "            continue\n",
    "        v5_id_start_stop_cache_file = f\"{v5_id_start_stop_data_dir}/{session_id}.csv\"\n",
    "        try:\n",
    "            if not os.path.exists(v5_id_start_stop_cache_file):\n",
    "                session_dir = frame_file_data[course][session_id]['dir_location']\n",
    "                frame_ids = frame_file_data[course][session_id]['frame_ids']\n",
    "                \n",
    "                session_tracking_cache_file = f\"{track_analysis_session_data}/{session_id}.pb\"\n",
    "                session_preprocessed_id_map_file = f\"{postprocessed_id_map_data_dir}/{session_id}.pb\"\n",
    "                session_eligible_pairs_map_file = f\"{embmatched_id_map_data_dir}/{session_id}.csv\"\n",
    "\n",
    "                printm(\"### get final id mapping for old to new ids with emb processing.\")\n",
    "                old_to_new_id_map = pickle.load(open(session_preprocessed_id_map_file,\"rb\"))\n",
    "                df_session_eligible_pairs = pd.read_csv(session_eligible_pairs_map_file)\n",
    "                \n",
    "                printm(\"### Correct new ids with dict from eligible pairs\")\n",
    "                eligible_id_map_dict = {}\n",
    "                for id_pair in df_session_eligible_pairs.id_pair.values:\n",
    "                    (id1, id2) = eval(id_pair)\n",
    "                    print(id1, id2)\n",
    "                    if id2 in eligible_id_map_dict:\n",
    "                        eligible_id_map_dict[id1] = eligible_id_map_dict[id2]\n",
    "                    else:\n",
    "                        eligible_id_map_dict[id2] = id1\n",
    "                \n",
    "                sprint(eligible_id_map_dict)\n",
    "                \n",
    "                printm(\"### correct created eligible map for once more\")\n",
    "                for key in sorted(list(eligible_id_map_dict.keys())):\n",
    "                    key_value = eligible_id_map_dict[key]\n",
    "                    if key_value in eligible_id_map_dict.keys():\n",
    "                        eligible_id_map_dict[key] = eligible_id_map_dict[key_value]\n",
    "                sprint(eligible_id_map_dict) \n",
    "                # sprint({kr:old_to_new_id_map[kr] for kr in old_to_new_id_map if (not old_to_new_id_map[kr]==10000)}) \n",
    "                \n",
    "                sprint(\"replacing ids in old to new id maps\")\n",
    "                for old_id in old_to_new_id_map:\n",
    "                    if old_to_new_id_map[old_id] in eligible_id_map_dict:\n",
    "                        print(f\"replacing {old_id}:{old_to_new_id_map[old_id]} -->{eligible_id_map_dict[old_to_new_id_map[old_id]]}\")\n",
    "                        old_to_new_id_map[old_id] = eligible_id_map_dict[old_to_new_id_map[old_id]]\n",
    "                # sprint({kr:old_to_new_id_map[kr] for kr in old_to_new_id_map if (not old_to_new_id_map[kr]==10000)}) \n",
    "\n",
    "                printm(\"### map new ids to tracking dataframe\")\n",
    "                df_tracking_new = pickle.load(open(session_tracking_cache_file,\"rb\")).transpose()\n",
    "                total_idxs = df_tracking_new.index.max()\n",
    "                for old_id in old_to_new_id_map:\n",
    "                    new_id = old_to_new_id_map[old_id]\n",
    "                    if not new_id==10000:\n",
    "                        new_id_col = f'N{new_id}'\n",
    "                        if new_id_col not in df_tracking_new:\n",
    "                            df_tracking_new[new_id_col] = None\n",
    "                        df_tracking_new[new_id_col] =  df_tracking_new[new_id_col].where(~df_tracking_new[new_id_col].isnull(), df_tracking_new[old_id])\n",
    "                    df_tracking_new = df_tracking_new.drop(old_id, axis=1)\n",
    "\n",
    "                printm(\"### get id start stop dataframe\")\n",
    "                col_start_stop_idxs = []\n",
    "                for col in df_tracking_new.columns:\n",
    "                    one_idxs = df_tracking_new.index[np.where(df_tracking_new[col]==1)[0]].values\n",
    "                    col_start_stop_idxs.append([col, one_idxs.min(), one_idxs.max()])\n",
    "                df_id_start_stop = pd.DataFrame(col_start_stop_idxs, columns=['id','min_idx','max_idx'])\n",
    "                df_id_start_stop['total_idxs'] = df_id_start_stop['max_idx']-df_id_start_stop['min_idx']\n",
    "                df_id_start_stop['id'] = df_id_start_stop['id'].apply(lambda x: int(x[1:]))\n",
    "                df_id_start_stop.to_csv(v5_id_start_stop_cache_file, index=False)\n",
    "                printm(f\"## Got id start stop info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            else:\n",
    "                ...\n",
    "                printm(f\"### FILE EXISTS: id start stop info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "        except:\n",
    "            printm(f\"## ERROR: Unable to get id start stop info for: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            unfinished_sessions.append((course, session_id))\n",
    "            print(traceback.format_exc())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df68c16-2663-4070-9a1e-e326a2bd9fec",
   "metadata": {},
   "source": [
    "# Get Groundtruth for course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6c934-0852-41cd-bbd5-ee701b791a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "course = '17214C'\n",
    "df_gt = pd.read_csv(f\"groundtruth/{course}.tsv\",sep='\\t',index_col=0)\n",
    "df_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4c3fd-04d6-4382-bafe-81da62434274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GT pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b543410-590d-4102-b719-0c820a674bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ground truth into gt_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f9406-feae-4e1e-bbdb-5bcd9358cb0c",
   "metadata": {},
   "source": [
    "# New algorithm for session pair id matching based on consistencies of ids present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e06e48-cdd9-4ee3-865c-e7e7618e4e57",
   "metadata": {},
   "source": [
    "## Describing the algorithm here.\n",
    "\n",
    "We are trying to find consistent ids across all sessions, and only do matching for those ids.\n",
    "\n",
    "- Step 1: filter out ids which are consistent more than 75% of the session from both sessions.\n",
    "- Step 2: Use 3 tiered rules to match ids across those sessions based on gaze and clu matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776756f8-5d42-4c20-9b94-2c93216c4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input for all sessions\n",
    "course_input_dict = {}\n",
    "idstartstop_input_dict = {}\n",
    "sessions = [xr for xr in session_filter_list if (course in xr)]\n",
    "for session in sessions:\n",
    "    session_input_file = f'{cross_session_input_data_dir}/{session}-front.pb'\n",
    "    session_id_start_stop_file = f'{v5_id_start_stop_data_dir}/{session}-front.csv'\n",
    "    course_input_dict[session] = pickle.load(open(session_input_file,\"rb\"))\n",
    "    idstartstop_input_dict[session] = pd.read_csv(session_id_start_stop_file)\n",
    "course_input_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c1c7a-d615-4524-a84d-6fcccd8f8e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e68825-04a3-4dac-94af-ae50556dd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_gt = deepcopy(df_gt)\n",
    "for col in df_filtered_gt:\n",
    "    df_filtered_gt[col] = df_filtered_gt[col].apply(lambda x: list(map(int,str(x).split(\",\"))) if not (x=='-1') else [])\n",
    "df_filtered_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ca178-f745-4600-972c-3dbfeb71413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_input_dict_raw= deepcopy(course_input_dict)\n",
    "\n",
    "for session in sessions:\n",
    "    df_id_start_stop_session = idstartstop_input_dict[session]\n",
    "    df_id_start_stop_session['presence_fraction'] = df_id_start_stop_session.total_idxs / df_id_start_stop_session.total_idxs.max()\n",
    "    filtered_ids = df_id_start_stop_session[df_id_start_stop_session.presence_fraction>0.5]['id'].values\n",
    "    course_input_dict[session] = {xr: course_input_dict[session][xr] for xr in course_input_dict[session] if xr in filtered_ids}\n",
    "    session_col = f'Session-{session[-8:-4]}'\n",
    "    df_filtered_gt[session_col] = df_filtered_gt[session_col].apply(lambda x: [xr for xr in x if (xr in filtered_ids)])\n",
    "    sprint(session, course_input_dict[session].keys())\n",
    "    # sprint(df_id_start_stop_session)\n",
    "    # _ = plt.figure(figsize=(20,15))\n",
    "    # for row_idx, row in df_id_start_stop_session.iterrows():\n",
    "    #     plt.axhline(y=row_idx, xmin=row['min_idx']/df_id_start_stop_session.total_idxs.max(),xmax=row['max_idx']/df_id_start_stop_session.total_idxs.max())\n",
    "    # plt.yticks(range(df_id_start_stop_session.shape[0]), range(df_id_start_stop_session.shape[0]))\n",
    "    # plt.grid() \n",
    "    sprint(filtered_ids)\n",
    "df_filtered_gt    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d0a5e-4802-4094-9072-f273013d3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get possible gt matches\n",
    "true_matches = []\n",
    "session_cols = list(df_filtered_gt.columns)\n",
    "for idx_colA in range(len(session_cols)):\n",
    "    for idx_colB in range(idx_colA+1, len(session_cols)):\n",
    "        match_val_pairs = zip(df_filtered_gt[session_cols[idx_colA]].values.tolist(), df_filtered_gt[session_cols[idx_colB]].values.tolist())\n",
    "        for ids_colA, ids_colB in match_val_pairs:\n",
    "            for id_colA in ids_colA:\n",
    "                for id_colB in ids_colB:\n",
    "                    session_keyA = session_cols[idx_colA].split(\"-\")[-1]\n",
    "                    session_keyB = session_cols[idx_colB].split(\"-\")[-1]\n",
    "                    true_matches.append([session_keyA, session_keyB, f'{session_keyA}_{id_colA}', f'{session_keyB}_{id_colB}'])\n",
    "df_true_matches = pd.DataFrame(true_matches, columns=['sessionA','sessionB','idA','idB'])\n",
    "df_true_matches['gt'] = 1\n",
    "df_true_matches             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98333128-8918-4768-9a3d-c1627e83d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_input_dict[session].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1b4d7-8def-40aa-8be3-62c6e4fb7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "type='constrained'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21abfc-5300-464f-9ac4-388e9d8ea7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# matching ids0.40\t0.3\t50\n",
    "# original 0.3, 0.2, 40\n",
    "MATCH_THRESHOLD=0.35\n",
    "MATCH_PAIR_MAX_THRESHOLD = 0.2\n",
    "MATCH_PAIR_AREA_THRESHOLD = 20\n",
    "\n",
    "# unconstrained\n",
    "if type=='unconstrained':\n",
    "    MATCH_THRESHOLD=0.35\n",
    "    MATCH_PAIR_MAX_THRESHOLD = 0.2\n",
    "    MATCH_PAIR_AREA_THRESHOLD = 20\n",
    "    print(type, MATCH_THRESHOLD, MATCH_PAIR_MAX_THRESHOLD, MATCH_PAIR_AREA_THRESHOLD)\n",
    "\n",
    "final_matches = []\n",
    "for (sessionA, sessionB) in product(sorted(sessions), sorted(sessions)):\n",
    "    sessionA_key, sessionB_key = sessionA.split(\"_\")[-1][4:8], sessionB.split(\"_\")[-1][4:8]\n",
    "    if not (sessionA==sessionB):\n",
    "        # if sessionA not in session_matches:\n",
    "        # match session A and session B based on gaze clustering\n",
    "        match_scores_gaze = {}\n",
    "        match_scores_clu  = {}\n",
    "        for idA,idB in product(course_input_dict[sessionA].keys(), course_input_dict[sessionB].keys()):\n",
    "\n",
    "            if not (type=='unconstrained'):\n",
    "                # check if there exists a gt match for idA in SessionB\n",
    "                idA_key = f'{sessionA_key}_{idA}'\n",
    "                idB_key = f'{sessionB_key}_{idB}'\n",
    "                idA_matches = df_true_matches[(df_true_matches.sessionB==sessionB_key) & (df_true_matches.idA==idA_key)].idB.values\n",
    "                if len(idA_matches)<=0:\n",
    "                    idA_matches = df_true_matches[(df_true_matches.sessionA==sessionB_key) & (df_true_matches.idB==idA_key)].idA.values\n",
    "                    if len(idA_matches)<=0:\n",
    "                        # print(f\"Skipping {idA},{idB} as no matches for {idA_key} in {sessionB_key}\")\n",
    "                        continue\n",
    "                idB_matches = df_true_matches[(df_true_matches.sessionA==sessionA_key) & (df_true_matches.idB==idB_key)].idA.values\n",
    "                if len(idB_matches) <= 0:\n",
    "                    idB_matches = df_true_matches[(df_true_matches.sessionB==sessionA_key) & (df_true_matches.idA==idB_key)].idB.values\n",
    "                    if len(idB_matches) <= 0:\n",
    "                        # print(f\"Skipping {idA},{idB} as no matches for {idB_key} in {sessionA_key}\")\n",
    "                        continue\n",
    "                \n",
    "            \n",
    "            gaze_embA, gaze_embB = course_input_dict[sessionA][idA]['gaze_emb'], course_input_dict[sessionB][idB]['gaze_emb']                \n",
    "            clu_embA, clu_embB = course_input_dict[sessionA][idA]['cluster_emb'], course_input_dict[sessionB][idB]['cluster_emb']                \n",
    "            \n",
    "            if idA not in match_scores_gaze:\n",
    "                match_scores_gaze[idA] = {}\n",
    "            if idA not in match_scores_clu:\n",
    "                match_scores_clu[idA] = {}\n",
    "                \n",
    "            if gaze_embA is None or gaze_embB is None:\n",
    "                match_scores_gaze[idA][idB] = np.inf\n",
    "            else:\n",
    "                match_distance = cdist(gaze_embA.reshape(1,-1), gaze_embB.reshape(1,-1))[0][0]\n",
    "                match_scores_gaze[idA][idB] = match_distance\n",
    "\n",
    "            if clu_embA is None or clu_embB is None:\n",
    "                match_scores_clu[idA][idB] = np.inf\n",
    "            else:\n",
    "                match_distance = cdist(clu_embA.reshape(1,-1), clu_embB.reshape(1,-1))[0][0]\n",
    "                match_scores_clu[idA][idB] = match_distance\n",
    "        \n",
    "        df_match_gaze = pd.DataFrame(match_scores_gaze) \n",
    "        df_match_clu = pd.DataFrame(match_scores_clu) \n",
    "        gaze_cols = df_match_gaze.columns.values.tolist()\n",
    "        clu_cols = df_match_clu.columns.values.tolist()\n",
    "        all_cols = np.unique(gaze_cols+clu_cols)\n",
    "        for col in all_cols:\n",
    "            if col not in df_match_clu.columns:\n",
    "                df_match_clu[col] = np.inf\n",
    "            if col not in df_match_gaze.columns:\n",
    "                df_match_clu[col] = np.inf\n",
    "                \n",
    "            sessionB_matches = deepcopy(df_match_clu[col]).sort_values().index.values.tolist() + \\\n",
    "                                deepcopy(df_match_gaze[col]).sort_values().index.values.tolist()\n",
    "            if type=='unconstrained':\n",
    "                sessionB_matches = deepcopy(df_match_clu[col]).sort_values().head(3).index.values.tolist() + \\\n",
    "                                deepcopy(df_match_gaze[col]).sort_values().head(3).index.values.tolist()\n",
    "            sessionB_matches = np.unique(sessionB_matches)\n",
    "            for match_id in sessionB_matches:\n",
    "                if (match_scores_clu[col][match_id]<MATCH_THRESHOLD) | (match_scores_gaze[col][match_id]<MATCH_THRESHOLD):\n",
    "                    col_face_area = course_input_dict[sessionA][col]['face_width_med'] * course_input_dict[sessionA][col]['face_height_med']\n",
    "                    match_face_area = course_input_dict[sessionB][match_id]['face_width_med'] * course_input_dict[sessionB][match_id]['face_height_med']\n",
    "                    rel_diff = np.abs(col_face_area-match_face_area)*100/min(col_face_area,match_face_area)\n",
    "                    final_matches.append((sessionA_key, sessionB_key, f'{sessionA_key}_{col}', f'{sessionB_key}_{match_id}', match_scores_gaze[col][match_id], match_scores_clu[col][match_id], col_face_area, match_face_area))\n",
    "\n",
    "df_final_matches = pd.DataFrame(final_matches, columns=['sessionA','sessionB','idA','idB','match_score_gaze','match_score_clu','face_areaA','face_areaB'])\n",
    "df_final_matches.info()\n",
    "df_final_matches.head(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecc9d6-393e-4133-9dd7-0a45812c6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_course_ids = {}\n",
    "session_pair_matches = {}\n",
    "session_keys = df_final_matches.sessionA.unique()\n",
    "\n",
    "for sessionA_idx in range(len(session_keys)):\n",
    "    for sessionB_idx in range(sessionA_idx+1, len(session_keys)):\n",
    "        # find all direct pairs\n",
    "        \n",
    "        direct_pairs = {}\n",
    "        s1, s2 = session_keys[sessionA_idx], session_keys[sessionB_idx]\n",
    "        df_pair_matches = df_final_matches[(df_final_matches.sessionA==s1) & (df_final_matches.sessionB==s2)]\n",
    "        df_pair_matches = df_pair_matches.sort_values(by=['face_areaA','face_areaB'],ascending=False)\n",
    "        df_pair_matches['area_diff'] = np.abs(df_pair_matches['face_areaA'] - df_pair_matches['face_areaB'])*100/np.minimum(df_pair_matches['face_areaA'],df_pair_matches['face_areaB'])\n",
    "        df_pair_matches['avg_match_score'] = (df_pair_matches['match_score_gaze'] + df_pair_matches['match_score_clu'])/2\n",
    "        # df_pair_matches['avg_match_score'] = df_pair_matches['match_score_gaze']\n",
    "        # df_pair_matches['avg_match_score'] = df_pair_matches['match_score_clu']\n",
    "        # get all eligible pairs with direct pairing\n",
    "        matched_idBs = []\n",
    "        for idA in df_pair_matches.idA.unique():\n",
    "            idA_matches = df_pair_matches[(df_pair_matches.idA==idA) & (~(df_pair_matches.idB.isin(matched_idBs)))].sort_values(by='avg_match_score')[['idB','avg_match_score','area_diff']].values\n",
    "            if idA_matches.shape[0] > 0:\n",
    "                matched_idB, match_score, area_diff = idA_matches[0][0],idA_matches[0][1],idA_matches[0][2]\n",
    "                matched_idBs.append(matched_idB)\n",
    "                direct_pairs[(idA,matched_idB)] = (match_score, area_diff)\n",
    "\n",
    "        inverse_pairs = {}\n",
    "        s1, s2 = session_keys[sessionB_idx], session_keys[sessionA_idx]\n",
    "        df_pair_matches = df_final_matches[(df_final_matches.sessionA==s1) & (df_final_matches.sessionB==s2)]\n",
    "        df_pair_matches = df_pair_matches.sort_values(by=['face_areaA','face_areaB'],ascending=False)\n",
    "        df_pair_matches['area_diff'] = np.abs(df_pair_matches['face_areaA'] - df_pair_matches['face_areaB'])*100/np.minimum(df_pair_matches['face_areaA'],df_pair_matches['face_areaB'])\n",
    "        df_pair_matches['avg_match_score'] = (df_pair_matches['match_score_gaze'] + df_pair_matches['match_score_clu'])/2\n",
    "        # df_pair_matches['avg_match_score'] = df_pair_matches['match_score_gaze']\n",
    "        # df_pair_matches['avg_match_score'] = df_pair_matches['match_score_clu']\n",
    "        # get all eligible pairs with direct pairing\n",
    "        matched_idBs = []\n",
    "        for idA in df_pair_matches.idA.unique():\n",
    "            idA_matches = df_pair_matches[(df_pair_matches.idA==idA) & (~(df_pair_matches.idB.isin(matched_idBs)))].sort_values(by='avg_match_score')[['idB','avg_match_score','area_diff']].values\n",
    "            if idA_matches.shape[0] > 0:\n",
    "                matched_idB, match_score, area_diff = idA_matches[0][0],idA_matches[0][1],idA_matches[0][2]\n",
    "                matched_idBs.append(matched_idB)\n",
    "                inverse_pairs[(matched_idB,idA)] = (match_score, area_diff)\n",
    "\n",
    "        final_pairs = []\n",
    "        paired_idAs, paired_idBs = [],[]\n",
    "        for pair_key in direct_pairs:\n",
    "            if pair_key in inverse_pairs:\n",
    "                if (pair_key[0] not in paired_idAs) & (pair_key[0] not in paired_idBs):\n",
    "                    final_pairs.append((pair_key, direct_pairs[pair_key],inverse_pairs[pair_key]))\n",
    "                    paired_idAs.append(pair_key[0])\n",
    "                    paired_idBs.append(pair_key[1])\n",
    "            elif (direct_pairs[pair_key][0] < MATCH_PAIR_MAX_THRESHOLD) & (direct_pairs[pair_key][1] < MATCH_PAIR_AREA_THRESHOLD):\n",
    "                if (pair_key[0] not in paired_idAs) & (pair_key[0] not in paired_idBs):\n",
    "                    final_pairs.append((pair_key, direct_pairs[pair_key],-1))\n",
    "                    paired_idAs.append(pair_key[0])\n",
    "                    paired_idBs.append(pair_key[1])\n",
    "        \n",
    "        for pair_key in inverse_pairs:\n",
    "            if (pair_key not in direct_pairs) & (inverse_pairs[pair_key][0] < MATCH_PAIR_MAX_THRESHOLD) & (inverse_pairs[pair_key][1] < MATCH_PAIR_AREA_THRESHOLD):\n",
    "                if (pair_key[0] not in paired_idAs) & (pair_key[0] not in paired_idBs):\n",
    "                    final_pairs.append((pair_key, -1,inverse_pairs[pair_key]))\n",
    "                    paired_idAs.append(pair_key[0])\n",
    "                    paired_idBs.append(pair_key[1])\n",
    "        session_pair_matches[(session_keys[sessionA_idx], session_keys[sessionB_idx])] = final_pairs\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828a713-4c89-415c-b0ec-4c4714ea9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id_sets = []\n",
    "for session_pair in session_pair_matches:\n",
    "    id_pairs_info = session_pair_matches[session_pair]\n",
    "    for (id_pair, direct_match_score, inv_match_score) in id_pairs_info:\n",
    "        found_pair = False\n",
    "        for id_set in all_id_sets:\n",
    "            if (id_pair[0] in id_set) or (id_pair[1] in id_set):\n",
    "                found_pair =  True\n",
    "                id_set.add(id_pair[0])\n",
    "                id_set.add(id_pair[1])\n",
    "                break\n",
    "        # print(all_id_sets, id_pair, found_pair)        \n",
    "        if not found_pair:\n",
    "            new_set = set()\n",
    "            new_set.add(id_pair[0])\n",
    "            new_set.add(id_pair[1])\n",
    "            all_id_sets.append(new_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407e24c-f874-4225-92ae-60a76529a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pair = ('0424_5', '0501_11')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e13ab-3580-4518-837e-af80868af95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_pair_in_gt(df_gt, id_pair):\n",
    "    s1, id1 = id_pair[0].split(\"_\")\n",
    "    s2, id2 = id_pair[1].split(\"_\")\n",
    "    for gt_idx, gt_row in df_gt.iterrows():\n",
    "        s1_matches = list(map(float,str(gt_row[f'Session-{s1}']).split(\",\")))\n",
    "        s2_matches = list(map(float,str(gt_row[f'Session-{s2}']).split(\",\")))\n",
    "        if (float(id1) in s1_matches) & (float(id2) in s2_matches):\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f02d2e-dde6-4bee-b9a9-7a07659a3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_true_pairs, num_total_pairs = 0,0\n",
    "predicted_pairs = []\n",
    "for session_pair in session_pair_matches:\n",
    "    id_pairs_info = session_pair_matches[session_pair]\n",
    "    for (id_pair, direct_match_score, inv_match_score) in id_pairs_info:\n",
    "        if session_cols.index(f'Session-{id_pair[0].split(\"_\")[0]}') < session_cols.index(f'Session-{id_pair[1].split(\"_\")[0]}'):\n",
    "            predicted_pairs.append([id_pair[0],id_pair[1]])\n",
    "        else:\n",
    "            predicted_pairs.append([id_pair[1],id_pair[0]])\n",
    "        if id_pair_in_gt(df_gt, id_pair):\n",
    "            # print(\"True Pair\", (id_pair, direct_match_score, inv_match_score))\n",
    "            print(id_pair)\n",
    "            num_true_pairs+=1\n",
    "        else:\n",
    "            ...\n",
    "            # print(\"False Pair\", (id_pair, direct_match_score, inv_match_score))\n",
    "        num_total_pairs+=1\n",
    "df_predicted = pd.DataFrame(predicted_pairs, columns=['idA','idB'])\n",
    "df_predicted['pred']=1\n",
    "# df_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640b187-ab11-479c-a90d-cdd0f03ace1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_matches[df_true_matches.idA=='0410_2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9d5fb-076b-4908-aeab-c972a580c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775204e-cf6d-40e6-9c48-81c2dd6ea795",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_true_pairs, num_total_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5e214-43dc-48d7-aa3c-d3c45c762eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_matches = pd.merge(df_true_matches, df_predicted,on=['idA','idB'], how='outer')\n",
    "df_all_matches.loc[df_all_matches['gt'].isnull(),'gt'] = 0\n",
    "df_all_matches.loc[df_all_matches['pred'].isnull(),'pred'] = 0\n",
    "df_all_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604766da-87ed-4f88-9998-adf216f9913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_matches.to_csv(f\"case_studies/results/across_session_{type}_matches_{course}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a49d1-6bd7-41e0-a59e-db34b9df37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_matches[(df_all_matches.pred==1) & (df_all_matches['gt']==1)].shape, df_all_matches.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e77c5-293f-4e7e-8c98-53327f7d60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(df_all_matches['gt'], df_all_matches['pred'], average=None), recall_score(df_all_matches['gt'], df_all_matches['pred'], average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e27ac-a33b-4c58-901f-36f302f6311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(df_all_matches['gt'], df_all_matches['pred']), recall_score(df_all_matches['gt'], df_all_matches['pred'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d44d5-a541-4b29-b540-0400b10bb09f",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717c81e-d101-46b7-b36e-865e7e9d4f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold_optimization = []\n",
    "match_threshold_choices = np.arange(0.15,1,0.05)\n",
    "match_pair_max_threshold_choices = np.arange(0,0.5,0.05)\n",
    "match_pair_area_threshold_choices = np.arange(0,100,5)\n",
    "hyperparameter_results =[] \n",
    "\n",
    "for (MATCH_THRESHOLD, MATCH_PAIR_MAX_THRESHOLD, MATCH_PAIR_AREA_THRESHOLD) in product(match_threshold_choices, match_pair_max_threshold_choices, match_pair_area_threshold_choices):\n",
    "    if MATCH_THRESHOLD<MATCH_PAIR_MAX_THRESHOLD:\n",
    "        continue\n",
    "    final_matches = []\n",
    "    for (sessionA, sessionB) in product(sorted(sessions), sorted(sessions)):\n",
    "        sessionA_key, sessionB_key = sessionA.split(\"_\")[-1][4:8], sessionB.split(\"_\")[-1][4:8]\n",
    "        if not (sessionA==sessionB):\n",
    "            # if sessionA not in session_matches:\n",
    "            # match session A and session B based on gaze clustering\n",
    "            match_scores_gaze = {}\n",
    "            match_scores_clu  = {}\n",
    "            for idA,idB in product(course_input_dict[sessionA].keys(), course_input_dict[sessionB].keys()):\n",
    "                # check if there exists a gt match for idA in SessionB\n",
    "                # idA_key = f'{sessionA_key}_{idA}'\n",
    "                # idB_key = f'{sessionB_key}_{idB}'\n",
    "                # idA_matches = df_true_matches[(df_true_matches.sessionB==sessionB_key) & (df_true_matches.idA==idA_key)].idB.values\n",
    "                # if len(idA_matches)<=0:\n",
    "                #     idA_matches = df_true_matches[(df_true_matches.sessionA==sessionB_key) & (df_true_matches.idB==idA_key)].idA.values\n",
    "                #     if len(idA_matches)<=0:\n",
    "                #         # print(f\"Skipping {idA},{idB} as no matches for {idA_key} in {sessionB_key}\")\n",
    "                #         continue\n",
    "                # idB_matches = df_true_matches[(df_true_matches.sessionA==sessionA_key) & (df_true_matches.idB==idB_key)].idA.values\n",
    "                # if len(idB_matches) <= 0:\n",
    "                #     idB_matches = df_true_matches[(df_true_matches.sessionB==sessionA_key) & (df_true_matches.idA==idB_key)].idB.values\n",
    "                #     if len(idB_matches) <= 0:\n",
    "                #         # print(f\"Skipping {idA},{idB} as no matches for {idB_key} in {sessionA_key}\")\n",
    "                #         continue\n",
    "\n",
    "                gaze_embA, gaze_embB = course_input_dict[sessionA][idA]['gaze_emb'], course_input_dict[sessionB][idB]['gaze_emb']                \n",
    "                clu_embA, clu_embB = course_input_dict[sessionA][idA]['cluster_emb'], course_input_dict[sessionB][idB]['cluster_emb']                \n",
    "                \n",
    "                if idA not in match_scores_gaze:\n",
    "                    match_scores_gaze[idA] = {}\n",
    "                if idA not in match_scores_clu:\n",
    "                    match_scores_clu[idA] = {}\n",
    "                    \n",
    "                if gaze_embA is None or gaze_embB is None:\n",
    "                    match_scores_gaze[idA][idB] = np.inf\n",
    "                else:\n",
    "                    match_distance = cdist(gaze_embA.reshape(1,-1), gaze_embB.reshape(1,-1))[0][0]\n",
    "                    match_scores_gaze[idA][idB] = match_distance\n",
    "    \n",
    "                if clu_embA is None or clu_embB is None:\n",
    "                    match_scores_clu[idA][idB] = np.inf\n",
    "                else:\n",
    "                    match_distance = cdist(clu_embA.reshape(1,-1), clu_embB.reshape(1,-1))[0][0]\n",
    "                    match_scores_clu[idA][idB] = match_distance\n",
    "            \n",
    "            df_match_gaze = pd.DataFrame(match_scores_gaze) \n",
    "            df_match_clu = pd.DataFrame(match_scores_clu) \n",
    "            gaze_cols = df_match_gaze.columns.values.tolist()\n",
    "            clu_cols = df_match_clu.columns.values.tolist()\n",
    "            all_cols = np.unique(gaze_cols+clu_cols)\n",
    "            for col in all_cols:\n",
    "                if col not in df_match_clu.columns:\n",
    "                    df_match_clu[col] = np.inf\n",
    "                if col not in df_match_gaze.columns:\n",
    "                    df_match_clu[col] = np.inf\n",
    "                sessionB_matches = deepcopy(df_match_clu[col]).sort_values().head(3).index.values.tolist() + \\\n",
    "                                    deepcopy(df_match_gaze[col]).sort_values().head(3).index.values.tolist()\n",
    "                sessionB_matches = np.unique(sessionB_matches)\n",
    "                for match_id in sessionB_matches:\n",
    "                    if (match_scores_clu[col][match_id]<MATCH_THRESHOLD) | (match_scores_gaze[col][match_id]<MATCH_THRESHOLD):\n",
    "                        col_face_area = course_input_dict[sessionA][col]['face_width_med'] * course_input_dict[sessionA][col]['face_height_med']\n",
    "                        match_face_area = course_input_dict[sessionB][match_id]['face_width_med'] * course_input_dict[sessionB][match_id]['face_height_med']\n",
    "                        rel_diff = np.abs(col_face_area-match_face_area)*100/min(col_face_area,match_face_area)\n",
    "                        final_matches.append((sessionA_key, sessionB_key, f'{sessionA_key}_{col}', f'{sessionB_key}_{match_id}', match_scores_gaze[col][match_id], match_scores_clu[col][match_id], col_face_area, match_face_area))\n",
    "    \n",
    "    df_final_matches = pd.DataFrame(final_matches, columns=['sessionA','sessionB','idA','idB','match_score_gaze','match_score_clu','face_areaA','face_areaB'])\n",
    "    \n",
    "    \n",
    "    final_course_ids = {}\n",
    "    session_pair_matches = {}\n",
    "    session_keys = df_final_matches.sessionA.unique()\n",
    "    \n",
    "    for sessionA_idx in range(len(session_keys)):\n",
    "        for sessionB_idx in range(sessionA_idx+1, len(session_keys)):\n",
    "            # find all direct pairs\n",
    "            \n",
    "            direct_pairs = {}\n",
    "            s1, s2 = session_keys[sessionA_idx], session_keys[sessionB_idx]\n",
    "            df_pair_matches = df_final_matches[(df_final_matches.sessionA==s1) & (df_final_matches.sessionB==s2)]\n",
    "            df_pair_matches = df_pair_matches.sort_values(by=['face_areaA','face_areaB'],ascending=False)\n",
    "            df_pair_matches['area_diff'] = np.abs(df_pair_matches['face_areaA'] - df_pair_matches['face_areaB'])*100/np.minimum(df_pair_matches['face_areaA'],df_pair_matches['face_areaB'])\n",
    "            df_pair_matches['avg_match_score'] = (df_pair_matches['match_score_gaze'] + df_pair_matches['match_score_clu'])/2\n",
    "            # df_pair_matches['avg_match_score'] = df_pair_matches['match_score_gaze']\n",
    "            # df_pair_matches['avg_match_score'] = df_pair_matches['match_score_clu']\n",
    "            # get all eligible pairs with direct pairing\n",
    "            matched_idBs = []\n",
    "            for idA in df_pair_matches.idA.unique():\n",
    "                idA_matches = df_pair_matches[(df_pair_matches.idA==idA) & (~(df_pair_matches.idB.isin(matched_idBs)))].sort_values(by='avg_match_score')[['idB','avg_match_score','area_diff']].values\n",
    "                if idA_matches.shape[0] > 0:\n",
    "                    matched_idB, match_score, area_diff = idA_matches[0][0],idA_matches[0][1],idA_matches[0][2]\n",
    "                    matched_idBs.append(matched_idB)\n",
    "                    direct_pairs[(idA,matched_idB)] = (match_score, area_diff)\n",
    "    \n",
    "            inverse_pairs = {}\n",
    "            s1, s2 = session_keys[sessionB_idx], session_keys[sessionA_idx]\n",
    "            df_pair_matches = df_final_matches[(df_final_matches.sessionA==s1) & (df_final_matches.sessionB==s2)]\n",
    "            df_pair_matches = df_pair_matches.sort_values(by=['face_areaA','face_areaB'],ascending=False)\n",
    "            df_pair_matches['area_diff'] = np.abs(df_pair_matches['face_areaA'] - df_pair_matches['face_areaB'])*100/np.minimum(df_pair_matches['face_areaA'],df_pair_matches['face_areaB'])\n",
    "            df_pair_matches['avg_match_score'] = (df_pair_matches['match_score_gaze'] + df_pair_matches['match_score_clu'])/2\n",
    "            # df_pair_matches['avg_match_score'] = df_pair_matches['match_score_gaze']\n",
    "            # df_pair_matches['avg_match_score'] = df_pair_matches['match_score_clu']\n",
    "            # get all eligible pairs with direct pairing\n",
    "            matched_idBs = []\n",
    "            for idA in df_pair_matches.idA.unique():\n",
    "                idA_matches = df_pair_matches[(df_pair_matches.idA==idA) & (~(df_pair_matches.idB.isin(matched_idBs)))].sort_values(by='avg_match_score')[['idB','avg_match_score','area_diff']].values\n",
    "                if idA_matches.shape[0] > 0:\n",
    "                    matched_idB, match_score, area_diff = idA_matches[0][0],idA_matches[0][1],idA_matches[0][2]\n",
    "                    matched_idBs.append(matched_idB)\n",
    "                    inverse_pairs[(matched_idB,idA)] = (match_score, area_diff)\n",
    "    \n",
    "            final_pairs = []\n",
    "            paired_idAs, paired_idBs = [],[]\n",
    "            for pair_key in direct_pairs:\n",
    "                if pair_key in inverse_pairs:\n",
    "                    if (pair_key[0] not in paired_idAs) & (pair_key[0] not in paired_idBs):\n",
    "                        final_pairs.append((pair_key, direct_pairs[pair_key],inverse_pairs[pair_key]))\n",
    "                        paired_idAs.append(pair_key[0])\n",
    "                        paired_idBs.append(pair_key[1])\n",
    "                elif (direct_pairs[pair_key][0] < MATCH_PAIR_MAX_THRESHOLD) & (direct_pairs[pair_key][1] < MATCH_PAIR_AREA_THRESHOLD):\n",
    "                    if (pair_key[0] not in paired_idAs) & (pair_key[0] not in paired_idBs):\n",
    "                        final_pairs.append((pair_key, direct_pairs[pair_key],-1))\n",
    "                        paired_idAs.append(pair_key[0])\n",
    "                        paired_idBs.append(pair_key[1])\n",
    "            \n",
    "            for pair_key in inverse_pairs:\n",
    "                if (pair_key not in direct_pairs) & (inverse_pairs[pair_key][0] < MATCH_PAIR_MAX_THRESHOLD) & (inverse_pairs[pair_key][1] < MATCH_PAIR_AREA_THRESHOLD):\n",
    "                    if (pair_key[0] not in paired_idAs) & (pair_key[0] not in paired_idBs):\n",
    "                        final_pairs.append((pair_key, -1,inverse_pairs[pair_key]))\n",
    "                        paired_idAs.append(pair_key[0])\n",
    "                        paired_idBs.append(pair_key[1])\n",
    "            session_pair_matches[(session_keys[sessionA_idx], session_keys[sessionB_idx])] = final_pairs\n",
    "    \n",
    "    all_id_sets = []\n",
    "    for session_pair in session_pair_matches:\n",
    "        id_pairs_info = session_pair_matches[session_pair]\n",
    "        for (id_pair, direct_match_score, inv_match_score) in id_pairs_info:\n",
    "            found_pair = False\n",
    "            for id_set in all_id_sets:\n",
    "                if (id_pair[0] in id_set) or (id_pair[1] in id_set):\n",
    "                    found_pair =  True\n",
    "                    id_set.add(id_pair[0])\n",
    "                    id_set.add(id_pair[1])\n",
    "                    break\n",
    "            # print(all_id_sets, id_pair, found_pair)        \n",
    "            if not found_pair:\n",
    "                new_set = set()\n",
    "                new_set.add(id_pair[0])\n",
    "                new_set.add(id_pair[1])\n",
    "                all_id_sets.append(new_set)\n",
    "    \n",
    "    num_true_pairs, num_total_pairs = 0,0\n",
    "    for session_pair in session_pair_matches:\n",
    "        id_pairs_info = session_pair_matches[session_pair]\n",
    "        for (id_pair, direct_match_score, inv_match_score) in id_pairs_info:\n",
    "            if id_pair_in_gt(df_gt, id_pair):\n",
    "                # print(\"True Pair\", (id_pair, direct_match_score, inv_match_score))\n",
    "                num_true_pairs+=1\n",
    "            # else:\n",
    "            num_total_pairs+=1\n",
    "                # print(\"False Pair\", (id_pair, direct_match_score, inv_match_score))\n",
    "                \n",
    "            \n",
    "            \n",
    "    if num_total_pairs > 0:\n",
    "        hyperparameter_results.append([MATCH_THRESHOLD, MATCH_PAIR_MAX_THRESHOLD, MATCH_PAIR_AREA_THRESHOLD, num_true_pairs, num_total_pairs, num_true_pairs/num_total_pairs])\n",
    "        print(MATCH_THRESHOLD, MATCH_PAIR_MAX_THRESHOLD, MATCH_PAIR_AREA_THRESHOLD, num_true_pairs, num_total_pairs, num_true_pairs/num_total_pairs)\n",
    "    else:\n",
    "        print(MATCH_THRESHOLD, MATCH_PAIR_MAX_THRESHOLD, MATCH_PAIR_AREA_THRESHOLD, num_true_pairs, num_total_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1adfc-026d-4b29-800d-3c43871f4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(hyperparameter_results, columns=['max_thr','max_pair_thr','max_area_thr','num_true_pairs','num_total_pairs','precision'])\n",
    "df_results.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78906f2-94ec-4dbf-bf18-5b2d99e24a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results.num_total_pairs>=25].sort_values(by='precision',ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685baa0-9829-4cad-b4f0-7bb8564ed36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1,s2 = '0228','0205'\n",
    "# df_pair_matches = df_final_matches[(df_final_matches.sessionA==s1) & (df_final_matches.sessionB==s2)]\n",
    "# df_pair_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9353e-f47a-4cac-b7a2-0772017e96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pair_matches = df_pair_matches.sort_values(by=['face_areaA','face_areaB'],ascending=False)\n",
    "# df_pair_matches['area_diff'] = np.abs(df_pair_matches['face_areaA'] - df_pair_matches['face_areaB'])*100/np.minimum(df_pair_matches['face_areaA'],df_pair_matches['face_areaB'])\n",
    "# df_pair_matches['avg_match_score'] = (df_pair_matches['match_score_gaze'] + df_pair_matches['match_score_clu'])/2\n",
    "# df_pair_matches[['idA','idB','area_diff','avg_match_score']]\n",
    "# # df_pair_matches.sort_values(by='idA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525665f6-9e8f-453c-95e0-9398065db364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pair_matches.idA.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee37fe8-e48e-4402-9385-1f0388845d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final pairs based on df_pair_matches\n",
    "# final_pairs = {}\n",
    "# matched_idBs = []\n",
    "# pair_match_max=0.2\n",
    "# for idA in df_pair_matches.idA.unique():\n",
    "#     idA_matches = df_pair_matches[(df_pair_matches.idA==idA) & (~(df_pair_matches.idB.isin(matched_idBs)))].sort_values(by='avg_match_score')[['idB','avg_match_score']].values\n",
    "#     if idA_matches.shape[0] > 0:\n",
    "#         matched_idB, match_score = idA_matches[0][0],idA_matches[0][1]\n",
    "#         if match_score<pair_match_max:\n",
    "#             matched_idBs.append(matched_idB)\n",
    "#             print(idA,matched_idB,match_score)  \n",
    "#         else:\n",
    "#             #potential pair\n",
    "#             print(\"Potential Pair:\", idA,matched_idB,match_score)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfb741-73da-4deb-8dd3-c160b4076c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final pairs based on df_pair_matches\n",
    "# final_pairs = {}\n",
    "# matched_idBs = []\n",
    "# # pair_match_max=0.2\n",
    "# for idA in df_pair_matches.idA.unique():\n",
    "#     idA_matches = df_pair_matches[(df_pair_matches.idA==idA) & (~(df_pair_matches.idB.isin(matched_idBs)))].sort_values(by='avg_match_score')[['idB','avg_match_score']].values\n",
    "#     if idA_matches.shape[0] > 0:\n",
    "#         matched_idB, match_score = idA_matches[0][0],idA_matches[0][1]\n",
    "#         matched_idBs.append(matched_idB)\n",
    "#         print(idA,matched_idB,match_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16eae8c-8880-4a76-8efd-08180ec62a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1f3f8-d8b0-4d4a-9b53-b1fb0e4b7a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd2f13-3f42-45ad-b038-090e931ff404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc8735-0267-4ef0-8e46-9351eaa60a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair_matches[(df_pair_matches.match_score_gaze<0.2) & (df_pair_matches.match_score_clu<0.2) & (df_pair_matches.area_diff<20) & (np.minimum(df_pair_matches.face_areaA, df_pair_matches.face_areaB)>np.median(df_pair_matches.face_areaA))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788c6d0-6bf2-4af0-bd38-4831d06bbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_matches = {}\n",
    "matched_idA, matched_idB = [],[]\n",
    "\n",
    "best_match_1 = df_pair_matches[(df_pair_matches.match_score_gaze<0.2) & \n",
    "                                (df_pair_matches.match_score_clu<0.2) & \n",
    "                                (df_pair_matches.area_diff<20) & \n",
    "                                (np.minimum(df_pair_matches.face_areaA, df_pair_matches.face_areaB)>np.median(df_pair_matches.face_areaA))\n",
    "                                ].sort_values(by='match_score_clu')\n",
    "sprint(best_match_1)\n",
    "for idx,row in best_match_1.iterrows():\n",
    "    if row['idA'] in matched_idA:\n",
    "        continue\n",
    "    elif row['idB'] in matched_idB:\n",
    "        continue\n",
    "    else:\n",
    "        id_matches[row['idA']] = row['idB']\n",
    "        matched_idA.append(row['idA'])\n",
    "        matched_idB.append(row['idB'])\n",
    "df_next_matches = df_pair_matches[(~df_pair_matches.idA.isin(matched_idA)) & (~df_pair_matches.idB.isin(matched_idB))]\n",
    "id_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f19e0-d191-45b2-810a-30c5391c7844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_next_matches = pd.merge(df_next_matches, df_next_matches.groupby('idA',as_index=False)['idB'].count(), on='idA',suffixes=('','_count'))\n",
    "df_next_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5265702-78c9-4e3a-9730-78141046caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_matches = df_next_matches[df_next_matches.idB_count<=1]\n",
    "best_match_2 = df_single_matches[(df_single_matches.match_score_gaze<0.25) & \n",
    "                                (df_single_matches.match_score_clu<0.25) & \n",
    "                                # (df_single_matches.area_diff<20) & \n",
    "                                (np.minimum(df_single_matches.face_areaA, df_single_matches.face_areaB)>np.median(df_pair_matches.face_areaA))\n",
    "                                ].sort_values(by='match_score_gaze')\n",
    "sprint(best_match_2)\n",
    "for idx,row in best_match_2.iterrows():\n",
    "    if row['idA'] in matched_idA:\n",
    "        continue\n",
    "    elif row['idB'] in matched_idB:\n",
    "        continue\n",
    "    else:\n",
    "        id_matches[row['idA']] = row['idB']\n",
    "        matched_idA.append(row['idA'])\n",
    "        matched_idB.append(row['idB'])\n",
    "df_next_matches = df_pair_matches[(~df_pair_matches.idA.isin(matched_idA)) & (~df_pair_matches.idB.isin(matched_idB))]\n",
    "id_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a2add-586a-47c9-9b29-302fd56aedfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_next_matches = pd.merge(df_next_matches, df_next_matches.groupby('idB',as_index=False)['idA'].count(), on='idB',suffixes=('','_count'))\n",
    "df_next_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaadf0-0583-4290-8b5f-df669fa76082",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match_3 = df_next_matches[(df_next_matches.idA_count<=1) & \n",
    "                                (df_next_matches.area_diff<20)].sort_values(by='match_score_gaze')\n",
    "sprint(best_match_3)\n",
    "for idx,row in best_match_3.iterrows():\n",
    "    if row['idA'] in matched_idA:\n",
    "        continue\n",
    "    elif row['idB'] in matched_idB:\n",
    "        continue\n",
    "    else:\n",
    "        id_matches[row['idA']] = row['idB']\n",
    "        matched_idA.append(row['idA'])\n",
    "        matched_idB.append(row['idB'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517f79c-57a0-42bc-841b-f7277ef73fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_next_matches = df_pair_matches[(~df_pair_matches.idA.isin(matched_idA)) & (~df_pair_matches.idB.isin(matched_idB))]\n",
    "id_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1e29e-d691-4064-88c7-a33cf103b5ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_next_matches = pd.merge(df_next_matches, df_next_matches.groupby('idB',as_index=False)['idA'].count(), on='idB',suffixes=('','_count'))\n",
    "df_next_matches = pd.merge(df_next_matches, df_next_matches.groupby('idA',as_index=False)['idB'].count(), on='idA',suffixes=('','_count'))\n",
    "df_next_matches.sort_values(by='idA_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ec7f3-f7a5-48fa-aeef-77a91d200cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_next_matches = pd.merge(df_next_matches, df_next_matches.groupby('idA',as_index=False).agg({'match_score_clu':lambda x: 1.0 if (len(x)<2) else sorted(x)[1]-sorted(x)[0]}), on='idA',suffixes=('','_best_diff'))\n",
    "df_next_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2061885-705d-4833-b955-4f84c8428dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_next_matches = pd.merge(df_next_matches, df_next_matches.groupby('idA',as_index=False).agg({'match_score_clu':lambda x: sorted(x)[0]}), on='idA',suffixes=('','_min_match'))\n",
    "df_next_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba69299-0b79-492f-bdc5-b79d426aa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match_4 = df_next_matches[(df_next_matches.match_score_gaze<0.25) &\n",
    "                (df_next_matches.match_score_clu_best_diff>0.05) &\n",
    "                (df_next_matches.match_score_clu==df_next_matches.match_score_clu_min_match)].sort_values(by='match_score_clu')\n",
    "sprint(best_match_4)\n",
    "for idx,row in best_match_4.iterrows():\n",
    "    if row['idA'] in matched_idA:\n",
    "        continue\n",
    "    elif row['idB'] in matched_idB:\n",
    "        continue\n",
    "    else:\n",
    "        id_matches[row['idA']] = row['idB']\n",
    "        matched_idA.append(row['idA'])\n",
    "        matched_idB.append(row['idB'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b494c-d22e-4c27-82f8-a3081478733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_next_matches = df_pair_matches[(~df_pair_matches.idA.isin(matched_idA)) & (~df_pair_matches.idB.isin(matched_idB))]\n",
    "id_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1bdb42-4810-4c6e-9148-3302284f58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_next_matches = df_next_matches.sort_values(by=['face_areaA'],ascending=False)\n",
    "df_next_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bfb8b-8bb2-4bd5-9aa4-64e2ad77127c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a5a54-2c07-45f5-81b1-a3acb4242c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
