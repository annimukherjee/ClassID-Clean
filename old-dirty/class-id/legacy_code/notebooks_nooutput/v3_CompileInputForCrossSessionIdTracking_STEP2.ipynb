{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1bf94-25d4-4ed1-bddd-bc3309763692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "# import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\", context=\"paper\")\n",
    "from cycler import cycler\n",
    "import os, sys\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import combinations, product\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO as _BytesIO\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.metrics import *\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "import traceback\n",
    "from sympy import Point, Polygon\n",
    "from decorators import *\n",
    "from smartprint import smartprint as sprint\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import DBSCAN\n",
    "# import plotly\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "def printm(s): return display(Markdown(s))\n",
    "    \n",
    "SERVER_CACHE_DIR = '/mnt/ci-nas-cache/edulyzeV2/cache_compute_4/fixed_face'\n",
    "os.makedirs(SERVER_CACHE_DIR,exist_ok=True)\n",
    "\n",
    "track_analysis_meta_cache = f'{SERVER_CACHE_DIR}/analysis_tracking/meta_info'\n",
    "base_dir = '/mnt/ci-nas-cache/edulyzeV2/pose_face_gaze_emb_fixed_face/'\n",
    "\n",
    "track_analysis_session_data = f'{SERVER_CACHE_DIR}/analysis_tracking/session_tracking_info'\n",
    "os.makedirs(track_analysis_session_data,exist_ok=True)\n",
    "\n",
    "postprocessed_id_map_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/processed_id_maps'\n",
    "os.makedirs(postprocessed_id_map_data_dir, exist_ok=True)\n",
    "\n",
    "emb_analysis_session_data = f'{SERVER_CACHE_DIR}/analysis_emb/session_emb_info_new'\n",
    "os.makedirs(emb_analysis_session_data,exist_ok=True)\n",
    "\n",
    "embmatched_id_raw_data_dir = f'{SERVER_CACHE_DIR}/analysis_emb/embmatched_id_raw'\n",
    "os.makedirs(embmatched_id_raw_data_dir,exist_ok=True)\n",
    "\n",
    "embmatched_id_map_data_dir = f'{SERVER_CACHE_DIR}/analysis_tracking/embmatched_id_maps_new'\n",
    "os.makedirs(embmatched_id_map_data_dir, exist_ok=True)\n",
    "\n",
    "cross_session_input_data_dir = f'{SERVER_CACHE_DIR}/analysis_emb/cross_session_input'\n",
    "os.makedirs(cross_session_input_data_dir, exist_ok=True)\n",
    "\n",
    "id_viz_cache_root = f'{SERVER_CACHE_DIR}/analysis_emb/session_matching_info'\n",
    "os.makedirs(id_viz_cache_root, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75185159-ce0e-439f-b41d-fba37829e865",
   "metadata": {},
   "source": [
    "## Get frame file data for all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61657d24-cddc-4722-bbde-b99313f4c2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "frame_file_data = {}\n",
    "for course_idx, course_dir in enumerate(glob.glob(f\"{base_dir}/*\")):\n",
    "    course_name = course_dir.split(\"/\")[-1]\n",
    "    course_cache_file = f\"{track_analysis_meta_cache}/{course_name}\"\n",
    "    if os.path.exists(course_cache_file):\n",
    "        frame_file_data[course_name] = pickle.load(open(course_cache_file,\"rb\"))\n",
    "        continue\n",
    "    frame_file_data[course_name]={}\n",
    "        \n",
    "    for session_idx, session_dir in enumerate(glob.glob(f\"{course_dir}/*\")):\n",
    "        session_name = session_dir.split(\"/\")[-1]\n",
    "        frame_file_data[course_name][session_name] = {}\n",
    "        frame_files = glob.glob(f\"{session_dir}/*\")\n",
    "        frame_file_names = [xr.split(\"/\")[-1] for xr in frame_files]\n",
    "        if 'end.pb' in frame_file_names:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=True\n",
    "        else:\n",
    "            frame_file_data[course_name][session_name]['is_completed']=False            \n",
    "        frame_ids = [int(xr.split(\".\")[0]) for xr in frame_file_names if not (xr=='end.pb')]\n",
    "        frame_file_data[course_name][session_name]['frame_ids'] = sorted(frame_ids)\n",
    "        frame_file_data[course_name][session_name]['dir_location'] = session_dir\n",
    "        print(f\"Got metadata for course: {course_idx}-{course_name}, session:{session_idx}-{session_name}\")\n",
    "    pickle.dump(frame_file_data[course_name],open(course_cache_file,\"wb\")) \n",
    "        \n",
    "frame_file_data.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7649b-de86-4207-8ebb-02c9bd2065e3",
   "metadata": {},
   "source": [
    "# Build final loop for single session compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf998f-dc47-447f-b1a8-14d55d69057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_filter_list = [\n",
    " #    'classinsight-cmu_05681A_ghc_4301_201905011630',\n",
    " # 'classinsight-cmu_05681A_ghc_4301_201904171630',\n",
    " # 'classinsight-cmu_05681A_ghc_4301_201902201630',\n",
    " # 'classinsight-cmu_05681A_ghc_4301_201904101630',\n",
    " # 'classinsight-cmu_05681A_ghc_4301_201901231630',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201902251200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904081200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201905011200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904291200',\n",
    " # 'classinsight-cmu_05418A_ghc_4102_201904011200',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201902141630',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201904021630',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201902051630',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201902281630',\n",
    " 'classinsight-cmu_05748A_ghc_4101_201903071630',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904230930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201903260930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904160930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201904300930',\n",
    " # 'classinsight-cmu_21127J_ghc_4102_201903190930',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904151500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201902251500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904081500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201904221500',\n",
    " # 'classinsight-cmu_05410A_ghc_4301_201902181500',\n",
    "                       \n",
    " 'classinsight-cmu_17214B_ph_a21_201902271030',\n",
    " 'classinsight-cmu_17214B_ph_a21_201903061030',\n",
    " 'classinsight-cmu_17214B_ph_a21_201904031030',\n",
    " 'classinsight-cmu_17214B_ph_a21_201904101030',\n",
    " 'classinsight-cmu_17214B_ph_a21_201904241030',\n",
    " 'classinsight-cmu_17214C_ph_225b_201903201130',\n",
    " 'classinsight-cmu_17214C_ph_225b_201904101130',\n",
    " 'classinsight-cmu_17214C_ph_225b_201904171130',\n",
    " 'classinsight-cmu_17214C_ph_225b_201904241130',\n",
    " 'classinsight-cmu_17214C_ph_225b_201905011130',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201902111500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201903181500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904081500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904151500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201904221500',\n",
    " # 'classinsight-cmu_05410B_ghc_4211_201901281500'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e99bd2-1553-4b20-9e98-f889741c5dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writing a generic loop to get embedding info from all courses in frame file data\n",
    "MAX_GAZE_DEVIATION_DEG = 30\n",
    "MIN_EMBEDDING_FRAMES = 100\n",
    "CLU_EPS = 0.4\n",
    "CLU_MIN_PTS = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "for course_idx, course in enumerate(frame_file_data):\n",
    "    for session_idx, session_id in enumerate(frame_file_data[course]):\n",
    "        if session_id.split(\"-front\")[0] not in session_filter_list:\n",
    "            print(f\"Session {session_id} not in session filter list, skipping...\")\n",
    "            continue\n",
    "        cross_session_input_cache_file = f\"{cross_session_input_data_dir}/{session_id}.pb\"\n",
    "        try:\n",
    "            if not os.path.exists(cross_session_input_cache_file):\n",
    "                printm(f\"## Building cross session input info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "                session_emb_info = pickle.load(open(f'{emb_analysis_session_data}/{session_id}.pb','rb'))\n",
    "                session_id_map = pickle.load(open(f\"{postprocessed_id_map_data_dir}/{session_id}.pb\",\"rb\"))\n",
    "                df_session_eligible_pairs = pd.read_csv(f\"{embmatched_id_map_data_dir}/{session_id}.csv\")\n",
    "                \n",
    "                # Correct new ids with dict from eligible pairs\n",
    "                eligible_id_map_dict = {}\n",
    "                for id_pair in df_session_eligible_pairs.id_pair.values:\n",
    "                    (id1, id2) = eval(id_pair)\n",
    "                    print(id1, id2)\n",
    "                    if id2 in eligible_id_map_dict:\n",
    "                        eligible_id_map_dict[id1] = eligible_id_map_dict[id2]\n",
    "                    else:\n",
    "                        eligible_id_map_dict[id2] = id1\n",
    "                \n",
    "                sprint(eligible_id_map_dict)\n",
    "\n",
    "                # correct created eligible map for once more\n",
    "                for key in sorted(list(eligible_id_map_dict.keys())):\n",
    "                    key_value = eligible_id_map_dict[key]\n",
    "                    if key_value in eligible_id_map_dict.keys():\n",
    "                        eligible_id_map_dict[key] = eligible_id_map_dict[key_value]\n",
    "                sprint(\"Corrected\", eligible_id_map_dict) \n",
    "\n",
    "                sprint(\"replacing ids in old to new id maps\")\n",
    "                for old_id in session_id_map:\n",
    "                    if session_id_map[old_id] in eligible_id_map_dict:\n",
    "                        print(f\"replacing {old_id}:{session_id_map[old_id]} -->{eligible_id_map_dict[session_id_map[old_id]]}\")\n",
    "                        session_id_map[old_id] = eligible_id_map_dict[session_id_map[old_id]]\n",
    "                # sprint({kr:old_to_new_id_map[kr] for kr in old_to_new_id_map if (not old_to_new_id_map[kr]==10000)}) \n",
    "\n",
    "                # Replace raw ids with mapped ids after postprocessing for both sessions\n",
    "                session_emb_info = {\n",
    "                    xr:{\n",
    "                        session_id_map[yr]:session_emb_info[xr][yr] \n",
    "                            for yr in session_emb_info[xr] if not (session_id_map[yr]==10000)} for xr in session_emb_info}\n",
    "\n",
    "                # arrange info as per tracking id across both sessions\n",
    "                gaze_info = {}\n",
    "                emb_info = {}\n",
    "                bbox_info = {}\n",
    "                face_info = {}\n",
    "                for frame_number in session_emb_info:\n",
    "                    for trackId in session_emb_info[frame_number]:\n",
    "                        # trackId = trackId_old if (trackId_old not in eligible_id_map_dict.keys()) else eligible_id_map_dict[trackId_old]\n",
    "                        if trackId not in gaze_info:\n",
    "                            gaze_info[trackId] = []\n",
    "                            emb_info[trackId]=[]\n",
    "                            bbox_info[trackId] = []\n",
    "                            face_info[trackId] = []\n",
    "                        # get  gaze info\n",
    "                        try:\n",
    "                            id_bbox = session_emb_info[frame_number][trackId]['bbox']\n",
    "                            bbox_info[trackId].append([frame_number]+list(id_bbox))\n",
    "                \n",
    "                            id_face = session_emb_info[frame_number][trackId]['face'][0]\n",
    "                            face_info[trackId].append([frame_number]+list(id_face))\n",
    "                            \n",
    "                            pitch, roll, yaw= session_emb_info[frame_number][trackId]['rvec'][0]\n",
    "                            pitch, roll, yaw=np.rad2deg(pitch), np.rad2deg(roll), np.rad2deg(yaw)\n",
    "                            gaze_sx, gaze_sy, gaze_ex, gaze_ey = session_emb_info[frame_number][trackId]['gaze_2d'][0].flatten()\n",
    "                            gaze_info[trackId].append([frame_number, pitch, roll, yaw, gaze_sx, gaze_sy, gaze_ex, gaze_ey])\n",
    "                            face_emb = session_emb_info[frame_number][trackId]['face_embedding'].tolist()\n",
    "                            emb_info[trackId].append([frame_number]+face_emb)\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                for id in gaze_info:\n",
    "                    gaze_info[id] = pd.DataFrame(gaze_info[id], columns=['frame','pitch','roll','yaw','gaze_sx', 'gaze_sy', 'gaze_ex', 'gaze_ey']).set_index('frame')\n",
    "                    emb_info[id] =pd.DataFrame(emb_info[id], columns=['frame']+np.arange(512).tolist()).set_index('frame')\n",
    "                    bbox_info[id] = pd.DataFrame(bbox_info[id], columns=['frame']+np.arange(5).tolist()).set_index('frame')\n",
    "                    face_info[id] = pd.DataFrame(face_info[id], columns=['frame']+np.arange(15).tolist()).set_index('frame')\n",
    "                \n",
    "                sprint({xr:(gaze_info[xr].shape[0],emb_info[xr].shape[0], face_info[xr].shape[0], bbox_info[xr].shape[0]) for xr in emb_info})\n",
    "\n",
    "                # get gaze based embeddings\n",
    "                gaze_based_embeddings = {}\n",
    "                for sid in emb_info.keys():\n",
    "                    #filter correct frames\n",
    "                    frames = gaze_info[sid][\n",
    "                        (gaze_info[sid].yaw.abs()<MAX_GAZE_DEVIATION_DEG) & \n",
    "                        (gaze_info[sid].pitch.abs()<MAX_GAZE_DEVIATION_DEG) & \n",
    "                        (gaze_info[sid].roll.abs()<MAX_GAZE_DEVIATION_DEG)].index.values\n",
    "                \n",
    "                    num_frames = len(frames)\n",
    "                    if (num_frames<MIN_EMBEDDING_FRAMES):\n",
    "                        sprint(f\"Not sufficient frames to match {sid}:{len(frames)}\")\n",
    "                        continue\n",
    "                    #get id embeddings    \n",
    "                    median_emb = np.median(emb_info[sid].loc[frames],axis=0)\n",
    "                    sprint(f\"Got gaze embedding for {sid}.\")\n",
    "                    gaze_based_embeddings[sid]=median_emb\n",
    "\n",
    "                # # get cluster based embeddings\n",
    "                # cluster_based_emb = {}\n",
    "                # for sid in emb_info:\n",
    "                #     emb_clu = DBSCAN(min_samples=CLU_MIN_PTS, eps=CLU_EPS)\n",
    "                #     try:\n",
    "                #         if emb_info[sid].shape[0]>25000:\n",
    "                #             emb_clu.fit(emb_info[sid].sample(25000).values)    \n",
    "                #         else:\n",
    "                #             emb_clu.fit(emb_info[sid].values)\n",
    "                #     except:\n",
    "                #         emb_clu=None\n",
    "                #     if (emb_clu is None) or (max(emb_clu.labels_)<0):\n",
    "                #         sprint(f\"All frames are outliers, not proceeding with id {sid}\")\n",
    "                #         continue\n",
    "                #     best_cluster_id = pd.Series(emb_clu.labels_[emb_clu.labels_>=0]).value_counts().index[0]\n",
    "                #     frames = emb_info[sid].iloc[emb_clu.labels_==best_cluster_id].index.values\n",
    "                #     cluster_based_emb[sid] = np.median(emb_info[sid].loc[frames],axis=0)\n",
    "                #     sprint(f\"Got cluster embedding for {sid}\")\n",
    "\n",
    "                # get complete information needed for a given id for cross-session id tracking\n",
    "                session_id_info = {}\n",
    "                for sid in sorted(face_info.keys()):\n",
    "                    sample_df =deepcopy(bbox_info[sid])\n",
    "                    \n",
    "                    sample_df['bbox_width'] = sample_df[2]-sample_df[0]\n",
    "                    sample_df['bbox_height'] = sample_df[3]-sample_df[1]\n",
    "                    sample_df['bbox_x'] = (sample_df[2]+sample_df[0]) / 2\n",
    "                    sample_df['bbox_y'] = (sample_df[3]+sample_df[1]) / 2\n",
    "\n",
    "                    bbox_width_med, bbox_height_med = sample_df['bbox_width'].median(), sample_df['bbox_height'].median()\n",
    "                    bbox_width_iqd = sample_df['bbox_width'].quantile(0.75) - sample_df['bbox_width'].quantile(0.25)\n",
    "                    bbox_height_iqd = sample_df['bbox_height'].quantile(0.75) - sample_df['bbox_height'].quantile(0.25)\n",
    "                    bbox_x_med, bbox_y_med = sample_df['bbox_x'].median(), sample_df['bbox_x'].median()\n",
    "                    bbox_x_iqd = sample_df['bbox_x'].quantile(0.75) - sample_df['bbox_x'].quantile(0.25)\n",
    "                    bbox_y_iqd = sample_df['bbox_y'].quantile(0.75) - sample_df['bbox_y'].quantile(0.25)\n",
    "\n",
    "\n",
    "                \n",
    "                    sample_df =deepcopy(face_info[sid])\n",
    "                    sample_df['face_width'] = sample_df[2]-sample_df[0]\n",
    "                    sample_df['face_height'] = sample_df[3]-sample_df[1]\n",
    "                    sample_df['face_height'] = sample_df[3]-sample_df[1]\n",
    "                    sample_df['face_x'] = (sample_df[2]+sample_df[0]) / 2\n",
    "                    sample_df['face_y'] = (sample_df[3]+sample_df[1]) / 2\n",
    "                    \n",
    "                    face_width_med, face_height_med = sample_df['face_width'].median(), sample_df['face_height'].median()\n",
    "                    face_width_iqd = sample_df['face_width'].quantile(0.75) - sample_df['face_width'].quantile(0.25)\n",
    "                    face_height_iqd = sample_df['face_height'].quantile(0.75) - sample_df['face_height'].quantile(0.25)\n",
    "                    \n",
    "                    face_x_med, face_y_med = sample_df['face_x'].median(), sample_df['face_x'].median()\n",
    "                    face_x_iqd = sample_df['face_x'].quantile(0.75) - sample_df['face_x'].quantile(0.25)\n",
    "                    face_y_iqd = sample_df['face_y'].quantile(0.75) - sample_df['face_y'].quantile(0.25)\n",
    "                \n",
    "                    session_id_info[sid] = dict(bbox_width_med=bbox_width_med, bbox_height_med=bbox_height_med,bbox_width_iqd=bbox_width_iqd, bbox_height_iqd=bbox_height_iqd,\n",
    "                                         bbox_x_med=bbox_x_med, bbox_y_med=bbox_y_med, bbox_x_iqd=bbox_x_iqd, bbox_y_iqd=bbox_y_iqd, \n",
    "                                         face_width_med=face_width_med, face_height_med=face_height_med, face_width_iqd=face_width_iqd, face_height_iqd=face_height_iqd,\n",
    "                                         face_x_med=face_x_med, face_y_med=face_y_med, face_x_iqd=face_x_iqd, face_y_iqd=face_y_iqd, \n",
    "                                         cluster_emb = cluster_based_emb.get(sid, None), \n",
    "                                         gaze_emb=gaze_based_embeddings.get(sid, None))\n",
    "                \n",
    "                sprint(sid, face_width_med, face_width_iqd, face_height_med, face_height_iqd, face_x_med, face_x_iqd, face_y_med, face_y_iqd)\n",
    "                pickle.dump(session_id_info, open(cross_session_input_cache_file,'wb'))\n",
    "                printm(f\"### Got cross session input info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            else:\n",
    "                ...\n",
    "                printm(f\"## FILE EXISTS: cross session input info for session: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "        except:\n",
    "            print(f\"ERROR: Unable to get cross session input info for: {course_idx}-{course}, session:{session_idx}-{session_id}\")\n",
    "            print(traceback.format_exc())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831a29d-6dc0-43cf-9806-d9fd969d66df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
